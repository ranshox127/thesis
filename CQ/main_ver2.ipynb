{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 取得專案根目錄 (lib 的父目錄)\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# 將 lib 加入 Python 模組搜尋路徑\n",
    "sys.path.append(os.path.join(root_dir, \"lib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Optional\n",
    "import json\n",
    "\n",
    "\n",
    "class Node(TypedDict):\n",
    "    node_id: str\n",
    "    question: str\n",
    "    annotation: str\n",
    "\n",
    "\n",
    "class Edge(TypedDict):\n",
    "    start: str\n",
    "    end: str\n",
    "    annotation: str\n",
    "\n",
    "\n",
    "class Q_DAG:\n",
    "    def __init__(self):\n",
    "        self.nodes: List[Node] = []\n",
    "        self.edges: List[Edge] = []\n",
    "\n",
    "    def add_root(self, question: str) -> None:\n",
    "        root_node = Node(node_id=\"Q\", question=question, annotation=\"\")\n",
    "        self.nodes.append(root_node)\n",
    "\n",
    "    def get_node_question(self, node_id: str) -> Optional[Node]:\n",
    "        for node in self.nodes:\n",
    "            if node[\"node_id\"] == node_id:\n",
    "                return node[\"question\"]\n",
    "        return None\n",
    "\n",
    "    def derive_question(self, new_sub_question: str, edge_annotation: str, parent_id: str) -> tuple[str, str]:\n",
    "        \"\"\"\n",
    "        根據 parent_id，派生出一個新的子節點（question）。\n",
    "        - 自動產生子節點 id，格式為 Q.1、Q.1.1 等（依據 parent_id）。\n",
    "        - 如果該子問題已存在，就不重複建立節點，但可以重複建立「新的父邊」。\n",
    "        - 檢查是否已有相同的邊；如果有就跳過。\n",
    "        - 檢查是否會造成循環（違反 DAG）。\n",
    "\n",
    "        回傳子節點 id（不論新建或重用）。\n",
    "        \"\"\"\n",
    "        existing_ids = {node[\"node_id\"] for node in self.nodes}\n",
    "        parent_ids = {node[\"node_id\"] for node in self.nodes}\n",
    "\n",
    "        # 檢查 parent_id 是否存在\n",
    "        if parent_id not in parent_ids:\n",
    "            raise ValueError(f\"Parent node '{parent_id}' does not exist.\")\n",
    "\n",
    "        # 想添加的子問題是否存在\n",
    "        existing_node_id = None\n",
    "        for node in self.nodes:\n",
    "            if self._normalize(node[\"question\"]) == self._normalize(new_sub_question):\n",
    "                existing_node_id = node[\"node_id\"]\n",
    "                break\n",
    "\n",
    "        # 若子問題不存在，則建立新節點\n",
    "        if existing_node_id is None:\n",
    "            # 完全新節點，可直接加入，不需 cycle 檢查\n",
    "            base = parent_id\n",
    "            i = 1\n",
    "            while True:\n",
    "                new_id = f\"{base}.{i}\"\n",
    "                if new_id not in existing_ids:\n",
    "                    break\n",
    "                i += 1\n",
    "\n",
    "            self.nodes.append(\n",
    "                Node(node_id=new_id, question=new_sub_question, annotation=\"\"))\n",
    "            self.edges.append(\n",
    "                Edge(start=parent_id, end=new_id, annotation=edge_annotation))\n",
    "            return new_id, \"new node added\"\n",
    "\n",
    "        # 如果子問題已存在，則使用現有的 ID\n",
    "        else:\n",
    "            new_id = existing_node_id\n",
    "            # 檢查是否已有這條邊\n",
    "            for edge in self.edges:\n",
    "                if edge[\"start\"] == parent_id and edge[\"end\"] == new_id:\n",
    "                    raise ValueError(\n",
    "                        f\"Edge from '{parent_id}' to '{new_id}' already exists.\")\n",
    "\n",
    "            # 子節點已存在，需檢查加入這條邊會不會形成環\n",
    "            temp_edges = self.edges + \\\n",
    "                [Edge(start=parent_id, end=new_id, annotation=edge_annotation)]\n",
    "            if self._has_cycle(temp_edges):\n",
    "                raise ValueError(\n",
    "                    f\"Adding edge from '{parent_id}' to '{new_id}' would create a cycle.\")\n",
    "\n",
    "            self.edges.append(\n",
    "                Edge(start=parent_id, end=new_id, annotation=edge_annotation))\n",
    "            return new_id, \"new edge added\"\n",
    "\n",
    "    def _has_cycle(self, edges: List[Edge]) -> bool:\n",
    "        \"\"\"簡單的 DFS 來偵測是否有循環。\"\"\"\n",
    "\n",
    "        from collections import defaultdict, deque\n",
    "\n",
    "        graph = defaultdict(list)\n",
    "        for edge in edges:\n",
    "            graph[edge[\"start\"]].append(edge[\"end\"])\n",
    "\n",
    "        visited = set()\n",
    "        in_path = set()\n",
    "\n",
    "        def dfs(node):\n",
    "            if node in in_path:\n",
    "                return True\n",
    "            if node in visited:\n",
    "                return False\n",
    "            visited.add(node)\n",
    "            in_path.add(node)\n",
    "            for neighbor in graph.get(node, []):\n",
    "                if dfs(neighbor):\n",
    "                    return True\n",
    "            in_path.remove(node)\n",
    "            return False\n",
    "\n",
    "        for node in graph:\n",
    "            if dfs(node):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _normalize(self, q: str) -> str:\n",
    "        return q.lower().strip().rstrip(\"?。！？\")\n",
    "\n",
    "    def update_node(self, node_id: str, new_annotation: str) -> None:\n",
    "        \"\"\"\n",
    "        根據 node_id 更新該節點的 annotation。\n",
    "        - 檢查 node_id 是否存在。\n",
    "        \"\"\"\n",
    "        for node in self.nodes:\n",
    "            if node[\"node_id\"] == node_id:\n",
    "                node[\"annotation\"] = new_annotation\n",
    "                return\n",
    "\n",
    "        raise ValueError(f\"Node with id '{node_id}' not found.\")\n",
    "\n",
    "    def export_DAG(self) -> str:\n",
    "        \"\"\"\n",
    "        將目前 DAG（nodes 與 edges）以 dict 形式包裝並轉為 JSON 字串。\n",
    "        回傳格式：\n",
    "        {\n",
    "            \"nodes\": [...],\n",
    "            \"edges\": [...]\n",
    "        }\n",
    "        \"\"\"\n",
    "        dag_dict = {\n",
    "            \"nodes\": self.nodes,\n",
    "            \"edges\": self.edges\n",
    "        }\n",
    "        return json.dumps(dag_dict, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "from tools import search\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# 1247 tokens\n",
    "planner_system_prompt = \"\"\"You are a Planner Agent designed to reason through complex, open-ended, or ambiguous questions by constructing, reflecting on, and expanding a directed acyclic graph (DAG) of interrelated sub-questions. Your task is not simply to retrieve answers, but to actively explore the question space, refine your understanding, and make informed decisions about when the original question has been sufficiently addressed.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem Space Representation: The Question DAG\n",
    "\n",
    "The DAG is your evolving internal model of the problem. It represents your reasoning process — how the main question relates to sub-questions, intermediate knowledge, and reflections.\n",
    "Each node contains:\n",
    "- `node_id`: a unique identifier\n",
    "- `question`: a sub-question or original question\n",
    "- `annotation`: your current thoughts, insights, summaries, or hypotheses about that question\n",
    "\n",
    "Each annotation helps build and maintain your internal representation of the problem. For example:\n",
    "- A node’s `annotation` may include:\n",
    "  - A summary of what you currently understand about the question\n",
    "  - A hypothesis or assumption you are testing\n",
    "  - A brief note on what you still need to find out\n",
    "- An `edge_annotation` should briefly explain how the sub-question contributes to answering the parent question — e.g., cause-effect, component, condition, clarification, definition, comparison, or implication.\n",
    "---\n",
    "\n",
    "## Input Format\n",
    "\n",
    "You are always shown the current DAG in JSON format, including all nodes and edges, representing the most up-to-date state of your reasoning process.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Reasoning Guidelines\n",
    "\n",
    "- You cannot delete nodes or edges. Even if a previous path turns out to be incorrect or irrelevant, leave it intact and revise your understanding through `update()`. This mimics how humans preserve earlier lines of thought for traceability, reflection, and learning from missteps.\n",
    "- You are encouraged to **revisit and revise** previous thoughts using `update`, especially as new information or sub-answers emerge.\n",
    "- When decomposing, focus on asking the right questions — use logical, causal, definitional, or investigative angles that deepen your understanding.\n",
    "- When unsure or the question is broad, **start by clarifying or framing the problem**, not jumping to answers.\n",
    "- For vague or ill-defined questions, take initiative to deconstruct ambiguity, identify what is missing, and reframe as needed. You shape the problem space.\n",
    "\n",
    "---\n",
    "\n",
    "## Your Tools\n",
    "\n",
    "You have three core actions to build and navigate the problem space:\n",
    "\n",
    "1. **question_decompose**\n",
    "   Use this to break down a question node into one or more meaningful sub-questions.\n",
    "   - You may decompose multiple nodes at once.\n",
    "   - Specify `parent_question_id`, `sub_question`, and an `edge_annotation` explaining the logical or conceptual relationship.\n",
    "   - Multiple parents pointing to the same sub-question are allowed.\n",
    "   - Keep the graph acyclic.\n",
    "\n",
    "   Example:\n",
    "   ```json\n",
    "   {\n",
    "     \"graph\": [\n",
    "       {\n",
    "         \"parent_question_id\": \"Q\",\n",
    "         \"sub_question\": \"How has telework affected work-life boundaries?\",\n",
    "         \"edge_annotation\": \"Understanding personal impact helps assess broader social shifts.\"\n",
    "       },\n",
    "       {\n",
    "         \"parent_question_id\": \"Q.1\",\n",
    "         \"sub_question\": \"Does telework reinforce or reduce social inequality?\",\n",
    "         \"edge_annotation\": \"Social impact includes distributional effects across groups.\"\n",
    "       }\n",
    "     ]\n",
    "   }\n",
    "    ```\n",
    "\n",
    "2.  **update**\n",
    "    Use this to revise or expand the annotation of existing nodes.\n",
    "    - This reflects new insights, summaries, clarifications, or changes in understanding.\n",
    "    - You are encouraged to use this tool to reflect, correct, or reframe — especially after learning something new.\n",
    "    - This is a key part of your **metacognitive behavior** — thinking about your thinking.\n",
    "    \n",
    "    Example:\n",
    "    ```json\n",
    "    {\n",
    "      \"nodes\": [\n",
    "        {\n",
    "          \"question_id\": \"Q.1\",\n",
    "          \"new_annotation\": \"Workers report blurred boundaries between home and work, leading to both flexibility and stress.\"\n",
    "        },\n",
    "        {\n",
    "          \"question_id\": \"Q.2\",\n",
    "          \"new_annotation\": \"Emerging evidence suggests that higher-income workers benefit more from telework options, widening inequality.\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    ```\n",
    "\n",
    "3.  **final_answer**\n",
    "    Use this only when you believe the original question has been sufficiently addressed **given the available steps so far**.  \n",
    "    You do not need perfect certainty — you must simply provide a reason why the current DAG gives you enough understanding to form a meaningful answer.\n",
    "    - Provide a justification explaining why you believe your DAG now contains enough understanding.\n",
    "    - Your answer should be clear, comprehensive, and informative—sufficient in length to convey key insights.\n",
    "    - You may use paragraph or bullet point format as appropriate.\n",
    "    - Aim to include key aspects uncovered in the DAG — such as causes, mechanisms, consequences, or trade-offs — without repeating every detail.\n",
    "    \n",
    "    Example:\n",
    "    ```json\n",
    "    {\n",
    "      \"reason\": \"The sub-questions cover key social dimensions — lifestyle, geography, and inequality — and their annotations provide sufficient insight.\",\n",
    "    }\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "## Metacognitive Expectations\n",
    "\n",
    "This is not a static search task — it is an evolving thinking process.\n",
    "\n",
    "- Use `update()` to **reflect**, summarize new insights, question assumptions, or refine your current framing.\n",
    "- Use `question_decompose()` to **expand the problem space**, identify what needs to be known, or clarify uncertainty.\n",
    "- Use `final_answer()` only when your internal model (the DAG) gives you enough confidence that you can answer well.\n",
    "- At each step, treat the DAG as your evolving internal model of understanding — be thoughtful about how you build it.\n",
    "\n",
    "- When starting from a single root question with no sub-questions yet, you may choose to either:\n",
    "  - Use `update()` to record your initial thoughts, assumptions, or possible lines of inquiry, or\n",
    "  - Use `question_decompose()` to begin breaking down the problem into more specific components.\n",
    "There is no fixed preference — use your best judgment based on the question’s clarity and complexity.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CQ_Solver:\n",
    "    def __init__(self, llm, system_prompt, max_turns=9, debug_log=\"CQ_Solver_debug.log\", summary_json=\"CQ_Solver_summary.json\"):\n",
    "        self.planner = Agent(llm=llm)\n",
    "        self.searcher = Agent(llm=llm)\n",
    "        self.max_turns = max_turns\n",
    "        self.planner_conversation = [\n",
    "            {\"role\": \"developer\", \"content\": system_prompt}]\n",
    "        self.conversation_log = []  # 用於詳細記錄每一條訊息，不做傳入模型用\n",
    "        self.total_tokens = []  # 用於詳細記錄每個response的tokens數量\n",
    "        self.DAG = Q_DAG()\n",
    "\n",
    "        # Setup detailed debug logging\n",
    "        logging.basicConfig(filename=debug_log, level=logging.DEBUG,\n",
    "                            format=\"%(asctime)s [%(levelname)s] %(message)s\", encoding=\"utf-8\")\n",
    "        logging.info(\"\\n=== New ReAct Execution Started ===\\n\")\n",
    "\n",
    "        # Summary log file\n",
    "        self.summary_json = summary_json\n",
    "\n",
    "        # Initialize JSON file if it doesn't exist\n",
    "        if not os.path.exists(self.summary_json):\n",
    "            with open(self.summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump([], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        self.tools = [{\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"question_decompose\",\n",
    "                \"description\": (\n",
    "                    \"Decompose an existing question node into one or more sub-questions, each linked to a parent node with an explanation. \"\n",
    "                    \"Based on your current understanding, generate new sub-questions from existing ones. \"\n",
    "                    \"Each new sub-question will trigger an automatic information retrieval process — relevant information or knowledge will be gathered and stored in the sub-question's annotation.\"\n",
    "                ),\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"graph\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"description\": \"A list of sub-question entries to add to the DAG, each linked to an existing parent node.\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"parent_question_id\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"The ID of an existing question node that serves as the parent.\"\n",
    "                                    },\n",
    "                                    \"sub_question\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"A sub-question derived from the parent question, based on your reasoning.\"\n",
    "                                    },\n",
    "                                    \"edge_annotation\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": (\n",
    "                                            \"An explanation of how this sub-question relates to its parent, or why asking it helps make progress. \"\n",
    "                                            \"This can reflect logical connections, assumptions, or any other explanation you find appropriate.\"\n",
    "                                        )\n",
    "                                    }\n",
    "                                },\n",
    "                                \"required\": [\"parent_question_id\", \"sub_question\", \"edge_annotation\"],\n",
    "                                \"additionalProperties\": False\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"graph\"],\n",
    "                    \"additionalProperties\": False\n",
    "                },\n",
    "                \"strict\": True\n",
    "            }\n",
    "        }, {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"update\",\n",
    "                \"description\": (\n",
    "                    \"Update the annotation field of one or more existing question nodes in the DAG. \"\n",
    "                    \"Use this when your understanding has evolved and you want to refine or correct previous annotations.\"\n",
    "                ),\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"nodes\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"description\": \"A list of annotation updates, each specifying a node's ID and the new content.\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"question_id\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"The ID of the question node to update.\"\n",
    "                                    },\n",
    "                                    \"new_annotation\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"The updated annotation for this node.\"\n",
    "                                    }\n",
    "                                },\n",
    "                                \"required\": [\"question_id\", \"new_annotation\"],\n",
    "                                \"additionalProperties\": False\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"nodes\"],\n",
    "                    \"additionalProperties\": False\n",
    "                },\n",
    "                \"strict\": True\n",
    "            }\n",
    "        }, {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"final_answer\",\n",
    "                \"description\": (\n",
    "                    \"Based on the current information and your confidence, explain why a final answer can be generated.\"\n",
    "                ),\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"reason\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Explain why you now have enough information to provide a final answer.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"reason\"],\n",
    "                    \"additionalProperties\": False\n",
    "                },\n",
    "                \"strict\": True\n",
    "            }\n",
    "        }]\n",
    "\n",
    "    def handle_tool_call(self, tool_call):\n",
    "        \"\"\"Executes the function requested by OpenAI's function calling system.\"\"\"\n",
    "        function_name = tool_call.function.name\n",
    "\n",
    "        try:\n",
    "            arguments = json.loads(tool_call.function.arguments)\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.warning(f\"JSON Decode Error in tool_call arguments: {e}\")\n",
    "            return \"retry\", None\n",
    "\n",
    "        logging.info(f\"Tool called: {function_name} with args: {arguments}\")\n",
    "\n",
    "        if function_name == \"question_decompose\":\n",
    "            graph_entries = arguments.get(\"graph\", [])\n",
    "            new_ids = []\n",
    "\n",
    "            for entry in graph_entries:\n",
    "                parent_id = entry.get(\"parent_question_id\")\n",
    "                sub_question = entry.get(\"sub_question\")\n",
    "                edge_annotation = entry.get(\"edge_annotation\")\n",
    "\n",
    "                try:\n",
    "                    new_id, status = self.DAG.derive_question(\n",
    "                        new_sub_question=sub_question,\n",
    "                        edge_annotation=edge_annotation,\n",
    "                        parent_id=parent_id\n",
    "                    )\n",
    "                except ValueError as e:\n",
    "                    logging.warning(str(e))\n",
    "                    continue\n",
    "\n",
    "                # 確定新增成功後再做後續\n",
    "                new_ids.append(new_id)\n",
    "\n",
    "                if status == \"new node added\":\n",
    "\n",
    "                    sub_node_info = self.get_sub_node_info(\n",
    "                        root_question=self.DAG.get_node_question(\"Q\"),\n",
    "                        parent_question=self.DAG.get_node_question(parent_id),\n",
    "                        annotation=edge_annotation,\n",
    "                        target_question=sub_question)\n",
    "\n",
    "                    self.DAG.update_node(new_id, sub_node_info)\n",
    "\n",
    "            if not new_ids:\n",
    "                return \"retry\", None\n",
    "\n",
    "            return \"decompose\", self.DAG.export_DAG()\n",
    "\n",
    "        elif function_name == \"update\":\n",
    "            nodes = arguments.get(\"nodes\", [])\n",
    "\n",
    "            for entry in nodes:\n",
    "                question_id = entry.get(\"question_id\")\n",
    "                new_annotation = entry.get(\"new_annotation\")\n",
    "                try:\n",
    "                    self.DAG.update_node(\n",
    "                        node_id=question_id, new_annotation=new_annotation)\n",
    "                except ValueError as e:\n",
    "                    logging.warning(str(e))\n",
    "                    continue\n",
    "\n",
    "            return \"update\", self.DAG.export_DAG()\n",
    "\n",
    "        elif function_name == \"search\":\n",
    "            query = arguments.get(\"query\")\n",
    "\n",
    "            logging.info(f\"Executing search for: {query}\")\n",
    "            return \"search\", search(query, max_results=5)\n",
    "\n",
    "        elif function_name == \"summary\":\n",
    "            summary = arguments.get(\"summary\")\n",
    "\n",
    "            logging.info(f\"summary: {summary}\")\n",
    "            return \"summary\", summary\n",
    "\n",
    "        elif function_name == \"final_answer\":\n",
    "            logging.info(\"Generating final answer...\")\n",
    "            return \"answer\", arguments.get(\"reason\")\n",
    "\n",
    "        else:\n",
    "            logging.warning(f\"Unknown function requested: {function_name}\")\n",
    "            return \"retry\", None\n",
    "\n",
    "    def get_sub_node_info(self, root_question, parent_question, annotation, target_question):\n",
    "        searcher_system_prompt = \"\"\"You are a research assistant in a multi-agent system designed to answer complex and ambiguous questions. Your role is to assist in answering sub-questions by generating search queries and summarizing relevant results.\n",
    "\n",
    "        You will be given:\n",
    "        - A root question: the user's original, high-level question\n",
    "        - A parent sub-question: a more specific inquiry derived from the root\n",
    "        - A target sub-question: the current question to be addressed\n",
    "        - An edge annotation: an explanation of how the target sub-question connects to its parent (i.e. the reasoning for asking it)\n",
    "\n",
    "        Your job consists of two steps:\n",
    "\n",
    "        1. **Query Generation**  \n",
    "        Based on the context (root question, parent question, edge annotation), write the most focused and effective search query to help retrieve useful information to address the target sub-question.  \n",
    "        - You are not merely rewriting the question. You must *interpret* the intent, especially if the edge annotation implies a deeper or more specific angle.\n",
    "        - For example, if the annotation indicates causal reasoning or a historical background is needed, reflect that in your query.\n",
    "\n",
    "        2. **Summary Generation**  \n",
    "        After receiving retrieved results, produce a concise but contextually appropriate summary that helps address the target sub-question.  \n",
    "        - The summary should match the *type of information* implied by the edge annotation.  \n",
    "        - Sometimes this may be a factual list, a comparison, a causal explanation, or a brief definition.  \n",
    "        - Avoid general or vague summaries; tailor the content to the sub-question's intent.\n",
    "\n",
    "        Be flexible: the edge annotation may imply different kinds of answers (e.g. factual, explanatory, evaluative), and your output should reflect that.\n",
    "\n",
    "        Use the available tools:\n",
    "        - `search(query, reason)`: to retrieve relevant information\n",
    "        - `summary(text)`: to return your synthesized summary\n",
    "        \"\"\"\n",
    "\n",
    "        searcher_conversation = [{\"role\": \"developer\", \"content\": searcher_system_prompt},\n",
    "                                 {\"role\": \"user\",\n",
    "                                  \"content\": (\n",
    "                                      f\"Root question: {root_question}\\n\"\n",
    "                                      f\"Parent question: {parent_question}\\n\"\n",
    "                                      f\"Target sub-question: {target_question}\\n\"\n",
    "                                      f\"Edge annotation: {annotation}\"\n",
    "                                  )\n",
    "                                  }]\n",
    "\n",
    "        tool = [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"search\",\n",
    "                    \"description\": \"Retrieve relevant web search results for a given query.\",\n",
    "                    \"strict\": True,\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"reason\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Reason why this search is needed.\"\n",
    "                            },\n",
    "                            \"query\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Search query string.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"reason\",\n",
    "                            \"query\"\n",
    "                        ],\n",
    "                        \"additionalProperties\": False\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"summary\",\n",
    "                    \"description\": \"summary retrieved data to answer the sub-question.\",\n",
    "                    \"strict\": True,\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"summary\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"summary of the retrieved data.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"summary\"],\n",
    "                        \"additionalProperties\": False\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        # search\n",
    "        response, usage = self.searcher.generate_response(\n",
    "            conversations=searcher_conversation, tools=tool, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"search\"}})\n",
    "\n",
    "        self.total_tokens.append({\n",
    "            \"prompt_tokens\": usage.prompt_tokens,\n",
    "            \"completion_tokens\": usage.completion_tokens,\n",
    "            \"total_tokens\": usage.total_tokens\n",
    "        })\n",
    "\n",
    "        search_call = response.tool_calls[0]\n",
    "\n",
    "        search_result = self.handle_tool_call(search_call)[1]\n",
    "\n",
    "        assistant_response = {\"role\": \"assistant\",\n",
    "                              \"tool_calls\": response.tool_calls}\n",
    "\n",
    "        tool_response = {\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": search_call.id,\n",
    "            \"content\": str(search_result)\n",
    "        }\n",
    "\n",
    "        searcher_conversation.append(assistant_response)\n",
    "        searcher_conversation.append(tool_response)\n",
    "\n",
    "        # summary\n",
    "\n",
    "        response, usage = self.searcher.generate_response(\n",
    "            conversations=searcher_conversation, tools=tool, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"summary\"}})\n",
    "\n",
    "        self.total_tokens.append({\n",
    "            \"prompt_tokens\": usage.prompt_tokens,\n",
    "            \"completion_tokens\": usage.completion_tokens,\n",
    "            \"total_tokens\": usage.total_tokens\n",
    "        })\n",
    "\n",
    "        summary_call = response.tool_calls[0]\n",
    "        summary_result = self.handle_tool_call(summary_call)[1]\n",
    "\n",
    "        self.conversation_log.extend([\n",
    "            {\"agent\": \"searcher\", **msg} for msg in searcher_conversation\n",
    "        ])\n",
    "\n",
    "        assistant_response = {\"agent\": \"searcher\", \"role\": \"assistant\",\n",
    "                              \"tool_calls\": response.tool_calls}\n",
    "\n",
    "        tool_response = {\n",
    "            \"agent\": \"searcher\",\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": summary_call.id,\n",
    "            \"content\": str(summary_result)\n",
    "        }\n",
    "\n",
    "        self.conversation_log.append(assistant_response)\n",
    "        self.conversation_log.append(tool_response)\n",
    "\n",
    "        return summary_result\n",
    "\n",
    "    def final_answer(self):\n",
    "        logging.info(\"Generating final answer...\")\n",
    "\n",
    "        response, usage = self.planner.generate_response(\n",
    "            self.planner_conversation, tools=self.tools, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"final_answer\"}})\n",
    "\n",
    "        self.total_tokens.append({\n",
    "            \"prompt_tokens\": usage.prompt_tokens,\n",
    "            \"completion_tokens\": usage.completion_tokens,\n",
    "            \"total_tokens\": usage.total_tokens\n",
    "        })\n",
    "\n",
    "        assistant_response = {\"agent\": \"planner\", \"role\": \"assistant\",\n",
    "                              \"tool_calls\": response.tool_calls}\n",
    "\n",
    "        self.planner_conversation.append(assistant_response)\n",
    "        self.conversation_log.append(assistant_response)\n",
    "\n",
    "        tool_call = response.tool_calls[0]\n",
    "        state, feedback = self.handle_tool_call(tool_call)\n",
    "\n",
    "        tool_response = {\n",
    "            \"agent\": \"planner\",\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "            \"content\": feedback,\n",
    "        }\n",
    "        self.planner_conversation.append(tool_response)\n",
    "        self.conversation_log.append(tool_response)\n",
    "\n",
    "    def _save_summary(self):\n",
    "        \"\"\"Saves the ReAct session to JSON with ordered retrieved data.\"\"\"\n",
    "        # If file is empty or invalid, initialize as empty list\n",
    "        if not os.path.exists(self.summary_json) or os.stat(self.summary_json).st_size == 0:\n",
    "            data = []\n",
    "        else:\n",
    "            try:\n",
    "                with open(self.summary_json, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)  # Load existing data\n",
    "            except json.JSONDecodeError:\n",
    "                logging.warning(\"JSON file is corrupted. Resetting to empty.\")\n",
    "                data = []  # Reset JSON if it's corrupted\n",
    "\n",
    "        # 把 message 中任何非可序列化的欄位轉換為字串或 dict\n",
    "        serializable_log = []\n",
    "\n",
    "        for msg in self.conversation_log:\n",
    "            if isinstance(msg, dict):\n",
    "                msg_copy = msg.copy()\n",
    "                if \"tool_calls\" in msg_copy:\n",
    "                    msg_copy[\"tool_calls\"] = [tc.model_dump() if hasattr(tc, \"model_dump\") else str(tc)\n",
    "                                              for tc in msg_copy[\"tool_calls\"]]\n",
    "                serializable_log.append(msg_copy)\n",
    "            else:\n",
    "                serializable_log.append(str(msg))\n",
    "\n",
    "        session_summary = {\n",
    "            \"question\": self.conversation_log[1][\"content\"].replace(\"Question: \", \"\"),\n",
    "            \"conversations\": serializable_log,\n",
    "            \"token_usage\": self.total_tokens\n",
    "        }\n",
    "\n",
    "        data.append(session_summary)\n",
    "\n",
    "        with open(self.summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False,\n",
    "                      indent=4)  # Save updated data\n",
    "\n",
    "    def run(self, question):\n",
    "        logging.info(f\"Starting new session with question: {question}\")\n",
    "\n",
    "        self.DAG.add_root(question)\n",
    "\n",
    "        user_question = {\"role\": \"user\", \"content\": self.DAG.export_DAG()}\n",
    "\n",
    "        self.planner_conversation.append(user_question)\n",
    "\n",
    "        self.conversation_log = [{\"agent\": \"planner\", **msg}\n",
    "                                 for msg in self.planner_conversation]\n",
    "\n",
    "        turn = 1\n",
    "\n",
    "        while turn <= self.max_turns:\n",
    "            logging.info(f\"Turn {turn}: Planner's action.\")\n",
    "\n",
    "            # 1. Generate Thought + Action\n",
    "            response, usage = self.planner.generate_response(\n",
    "                self.planner_conversation, tools=self.tools, tool_choice=\"required\")\n",
    "            logging.info(f\"LLM Response:\\n{response}\")\n",
    "\n",
    "            self.total_tokens.append({\n",
    "                \"prompt_tokens\": usage.prompt_tokens,\n",
    "                \"completion_tokens\": usage.completion_tokens,\n",
    "                \"total_tokens\": usage.total_tokens\n",
    "            })\n",
    "\n",
    "            # 2. Check if the LLM requested a function call\n",
    "            if response.tool_calls:\n",
    "                assistant_response = {\"agent\": \"planner\", \"role\": \"assistant\",\n",
    "                                      \"tool_calls\": response.tool_calls}\n",
    "\n",
    "                self.planner_conversation.append(assistant_response)\n",
    "                self.conversation_log.append(assistant_response)\n",
    "\n",
    "                for tool_call in response.tool_calls:\n",
    "                    state, feedback = self.handle_tool_call(tool_call)\n",
    "\n",
    "                    if state == \"retry\":\n",
    "                        logging.warning(f\"Retrying Turn {turn}...\")\n",
    "                        turn -= 1\n",
    "                        continue\n",
    "\n",
    "                    if state == \"decompose\":\n",
    "                        tool_response = {\n",
    "                            \"agent\": \"planner\",\n",
    "                            \"role\": \"tool\",\n",
    "                            \"tool_call_id\": tool_call.id,\n",
    "                            \"content\": feedback,\n",
    "                        }\n",
    "                        self.planner_conversation.append(tool_response)\n",
    "                        self.conversation_log.append(tool_response)\n",
    "\n",
    "                        logging.info(f\"Graph: {feedback}\")\n",
    "\n",
    "                        Critical_Evaluation_request = {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": (\n",
    "                                \"You have just decomposed part of the problem into new sub-questions. Now, take a moment to reflect on your current understanding and planning:\\n\\n\"\n",
    "                                \"1. Have the new sub-questions changed or expanded your understanding of the original question or any part of the problem space?\\n\"\n",
    "                                \"    - If yes, consider using `update()` to revise or refine your current annotations.\\n\\n\"\n",
    "                                \"2. Are there any remaining uncertainties, vague concepts, or areas that seem underdeveloped?\\n\"\n",
    "                                \"    - If yes, you may want to continue decomposing or exploring before concluding.\\n\\n\"\n",
    "                                \"3. If you believe you are ready to answer the original question, pause and verify your confidence:\\n\"\n",
    "                                \"    - Formulate **a few critical questions** that would challenge or test your current answer.\\n\"\n",
    "                                \"    - If your answer still holds after these checks, then proceed with `final_answer()`.\\n\"\n",
    "                                \"    - Otherwise, revise your thinking or explore further as needed.\\n\\n\"\n",
    "                                \"Choose your next tool based on your reflection.\"\n",
    "                            )\n",
    "                        }\n",
    "\n",
    "                        self.planner_conversation.append(\n",
    "                            Critical_Evaluation_request)\n",
    "                        self.conversation_log.append(\n",
    "                            Critical_Evaluation_request)\n",
    "\n",
    "                        response, usage = self.planner.generate_response(\n",
    "                            self.planner_conversation, tools=self.tools, tool_choice=\"none\")\n",
    "\n",
    "                        logging.info(f\"LLM Response:\\n{response.content}\")\n",
    "\n",
    "                        self.total_tokens.append({\"prompt_tokens\": usage.prompt_tokens,\n",
    "                                                  \"completion_tokens\": usage.completion_tokens,\n",
    "                                                  \"total_tokens\": usage.total_tokens})\n",
    "\n",
    "                        Critical_Evaluation_response = {\n",
    "                            \"agent\": \"planner\", \"role\": \"assistant\", \"content\": response.content}\n",
    "\n",
    "                        self.planner_conversation.append(\n",
    "                            Critical_Evaluation_response)\n",
    "                        self.conversation_log.append(\n",
    "                            Critical_Evaluation_response)\n",
    "\n",
    "                    if state == \"update\":\n",
    "                        tool_response = {\n",
    "                            \"agent\": \"planner\",\n",
    "                            \"role\": \"tool\",\n",
    "                            \"tool_call_id\": tool_call.id,\n",
    "                            \"content\": feedback,\n",
    "                        }\n",
    "                        self.planner_conversation.append(tool_response)\n",
    "                        self.conversation_log.append(tool_response)\n",
    "\n",
    "                        logging.info(f\"Graph: {feedback}\")\n",
    "\n",
    "                    elif state == \"answer\":\n",
    "                        tool_response = {\n",
    "                            \"agent\": \"planner\",\n",
    "                            \"role\": \"tool\",\n",
    "                            \"tool_call_id\": tool_call.id,\n",
    "                            \"content\": feedback,\n",
    "                        }\n",
    "                        self.planner_conversation.append(tool_response)\n",
    "                        self.conversation_log.append(tool_response)\n",
    "\n",
    "                        final_answer_request = {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": (\n",
    "                                \"Please organize the information you have gathered and write a complete and comprehensive answer to the original question.\")}\n",
    "\n",
    "                        self.planner_conversation.append(final_answer_request)\n",
    "                        self.conversation_log.append(final_answer_request)\n",
    "\n",
    "                        response, usage = self.planner.generate_response(\n",
    "                            self.planner_conversation, tools=self.tools, tool_choice=\"none\")\n",
    "\n",
    "                        logging.info(f\"LLM Response:\\n{response.content}\")\n",
    "                        self.total_tokens.append({\"prompt_tokens\": usage.prompt_tokens,\n",
    "                                                  \"completion_tokens\": usage.completion_tokens,\n",
    "                                                  \"total_tokens\": usage.total_tokens})\n",
    "\n",
    "                        self.conversation_log.append(\n",
    "                            {\"agent\": \"planner\", \"role\": \"assistant\", \"content\": response.content})\n",
    "\n",
    "                        self._save_summary()\n",
    "                        logging.info(\"Final Answer Reached.\")\n",
    "                        return response.content\n",
    "\n",
    "            turn += 1\n",
    "\n",
    "        logging.warning(\"Max turns reached. No definitive answer found.\")\n",
    "        self.final_answer()\n",
    "\n",
    "        final_answer_request = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Please organize the information you have gathered and write a complete and comprehensive answer to the original question.\")}\n",
    "\n",
    "        self.planner_conversation.append(final_answer_request)\n",
    "        self.conversation_log.append(final_answer_request)\n",
    "\n",
    "        response, usage = self.planner.generate_response(\n",
    "            self.planner_conversation, tools=self.tools, tool_choice=\"none\")\n",
    "\n",
    "        self.total_tokens.append({\"prompt_tokens\": usage.prompt_tokens,\n",
    "                                  \"completion_tokens\": usage.completion_tokens,\n",
    "                                  \"total_tokens\": usage.total_tokens})\n",
    "\n",
    "        self.conversation_log.append(\n",
    "            {\"agent\": \"planner\", \"role\": \"assistant\", \"content\": response.content})\n",
    "\n",
    "        self._save_summary()\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing https://www.thoughtco.com/utopian-movements-104221: HTTPSConnectionPool(host='www.thoughtco.com', port=443): Read timed out. (read timeout=5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"### Pros and Cons of Utopianism\\n\\nUtopianism represents a philosophical ideology centered around the vision of ideal societies that promise collective well-being, social justice, and equal opportunity. This concept has been explored through various lenses, leading to a rich discourse that highlights both the aspirational strengths and critical weaknesses of utopian thought.\\n\\n#### Pros of Utopianism\\n\\n1. **Inspiration for Social Reform**: Utopianism encourages individuals and communities to reimagine society beyond its current limitations, inspiring movements aimed at social justice, equality, and collective well-being. By providing a visionary contrast to existing systems, utopian ideals promote critical examination and advocate for meaningful reforms.\\n\\n2. **Promoting Equality and Justice**: The core tenets of many utopian visions include the pursuit of equity and the common good. These principles motivate individuals to work towards structures that are free from inequality, oppression, and violence, thus fostering a sense of community and shared purpose.\\n\\n3. **Transcending Conventional Problem-Solving**: Utopian aspirations often spur creative approaches to complex societal challenges. By focusing on long-term goals and imaginative frameworks, utopian thought can lead to innovative solutions and transformative practices that break away from entrenched ways of thinking.\\n\\n4. **Cultural Harmony and Tolerance**: Utopian narratives frequently explore themes of coexistence among diverse beliefs and practices, promoting moral and ethical dimensions that underscore the importance of tolerance and harmony in societal interactions.\\n\\n#### Cons of Utopianism\\n\\n1. **Potential for Authoritarianism**: Critics argue that strict adherence to utopian ideals can lead to authoritarian regimes, wherein dissent is suppressed in favor of a singular vision of society. This phenomenon can arise from what philosopher Karl Popper terms the 'blueprint' approach, which proposes a detailed model of the ideal society without considering the pluralistic nature of human desires.\\n\\n2. **History of Failed Experiments**: Historical attempts at establishing utopian societies, such as Brook Farm, New Harmony, and the Oneida Community, often resulted in disillusionment and collapse. Common reasons for failure include unrealistic expectations, economic viability issues, and the tendency for hierarchical power dynamics to emerge, undermining the egalitarian goals.\\n\\n3. **Conflicts Arising from Competing Ideals**: The plurality of human goals and values means that what is viewed as 'utopian' can differ vastly among individuals and groups. This diversity can lead to internal conflicts and struggles for dominance that destabilize the envisioned society, often resulting in violence or coercion to maintain a particular ideal.\\n\\n4. **Challenges of Human Nature**: Utopianism often assumes that individuals will naturally align with collective goals, overlooking innate human tendencies toward selfishness and competition. This oversight can contribute to significant friction within communities striving for radical societal change.\\n\\nIn conclusion, utopianism presents a dual-edged sword—a powerful source of inspiration and idealism that can motivate social progress, while simultaneously posing risks of authoritarianism, conflict, and practical failures. The discourse surrounding utopianism is essential for understanding not only its historical implications but also its relevance in contemporary discussions about societal structures and aspirations.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planner_system_prompt\n",
    "ms = CQ_Solver(llm=\"gpt-4o-mini\", system_prompt=planner_system_prompt, max_turns=9)\n",
    "ms.run(\"pros and cons of utopianism?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_prompt_tokens': 92040, 'total_completion_tokens': 3460, 'input_cost_ntd': 27.15, 'output_cost_ntd': 1.37, 'total_cost_ntd': 28.52}\n"
     ]
    }
   ],
   "source": [
    "def calculate_token_cost(token_usage_list, input_rate=2.5, output_rate=10, num_q=500):\n",
    "    \"\"\"\n",
    "    計算總成本（以新台幣計算），根據 token 使用量。\n",
    "\n",
    "    Args:\n",
    "        token_usage_list (list of dict): 每個 dict 包含 prompt_tokens, completion_tokens\n",
    "        input_rate (float): 輸入成本，單位為每 1M token 的新台幣（預設 2.5）\n",
    "        output_rate (float): 輸出成本，單位為每 1M token 的新台幣（預設 10）\n",
    "        num_q (int): 問多少道問題\n",
    "\n",
    "    Returns:\n",
    "        dict: 包含輸入、輸出、總成本與總 token\n",
    "    \"\"\"\n",
    "\n",
    "    total_prompt = sum(x[\"prompt_tokens\"] for x in token_usage_list)\n",
    "    total_completion = sum(x[\"completion_tokens\"] for x in token_usage_list)\n",
    "\n",
    "    input_cost = total_prompt * num_q * input_rate / 1000000\n",
    "    output_cost = total_completion * num_q * output_rate / 1000000\n",
    "    total_cost = input_cost + output_cost\n",
    "\n",
    "    return {\n",
    "        \"total_prompt_tokens\": total_prompt,\n",
    "        \"total_completion_tokens\": total_completion,\n",
    "        \"input_cost_ntd\": round(input_cost, 2),\n",
    "        \"output_cost_ntd\": round(output_cost, 2),\n",
    "        \"total_cost_ntd\": round(total_cost, 2)\n",
    "    }\n",
    "\n",
    "\n",
    "token_usage = [\n",
    "            {\n",
    "                \"prompt_tokens\": 1625,\n",
    "                \"completion_tokens\": 165,\n",
    "                \"total_tokens\": 1790\n",
    "            },\n",
    "            {\n",
    "                \"prompt_tokens\": 529,\n",
    "                \"completion_tokens\": 40,\n",
    "                \"total_tokens\": 569\n",
    "            },\n",
    "            {\n",
    "                \"prompt_tokens\": 12692,\n",
    "                \"completion_tokens\": 393,\n",
    "                \"total_tokens\": 13085\n",
    "            },\n",
    "            {\n",
    "                \"prompt_tokens\": 535,\n",
    "                \"completion_tokens\": 34,\n",
    "                \"total_tokens\": 569\n",
    "            },\n",
    "            {\n",
    "                \"prompt_tokens\": 3171,\n",
    "                \"completion_tokens\": 247,\n",
    "                \"total_tokens\": 3418\n",
    "            },\n",
    "            {\n",
    "                \"prompt_tokens\": 532,\n",
    "                \"completion_tokens\": 35,\n",
    "                \"total_tokens\": 567\n",
    "            },\n",
    "            {\n",
    "                \"prompt_tokens\": 11227,\n",
    "                \"completion_tokens\": 214,\n",
    "                \"total_tokens\": 11441\n",
    "            },\n",
    "            {\n",
    "                \"prompt_tokens\": 534,\n",
    "                \"completion_tokens\": 35,\n",
    "                \"total_tokens\": 569\n",
    "            },\n",
    "            {\n",
    "                \"prompt_tokens\": 3832,\n",
    "                \"completion_tokens\": 188,\n",
    "                \"total_tokens\": 4020\n",
    "            },\n",
    "            {\n",
    "                \"prompt_tokens\": 3348,\n",
    "                \"completion_tokens\": 190,\n",
    "                \"total_tokens\": 3538\n",
    "            },\n",
    "            {\n",
    "                \"prompt_tokens\": 3540,\n",
    "                \"completion_tokens\": 248,\n",
    "                \"total_tokens\": 3788\n",
    "            },\n",
    "            {\n",
    "                \"prompt_tokens\": 4310,\n",
    "                \"completion_tokens\": 99,\n",
    "                \"total_tokens\": 4409\n",
    "            },\n",
    "            {\n",
    "                \"prompt_tokens\": 536,\n",
    "                \"completion_tokens\": 48,\n",
    "                \"total_tokens\": 584\n",
    "            },\n",
    "            {\n",
    "                \"prompt_tokens\": 10013,\n",
    "                \"completion_tokens\": 207,\n",
    "                \"total_tokens\": 10220\n",
    "            },\n",
    "            {\n",
    "                \"prompt_tokens\": 535,\n",
    "                \"completion_tokens\": 34,\n",
    "                \"total_tokens\": 569\n",
    "            },\n",
    "            {\n",
    "                \"prompt_tokens\": 17033,\n",
    "                \"completion_tokens\": 285,\n",
    "                \"total_tokens\": 17318\n",
    "            },\n",
    "            {\n",
    "                \"prompt_tokens\": 5769,\n",
    "                \"completion_tokens\": 252,\n",
    "                \"total_tokens\": 6021\n",
    "            },\n",
    "            {\n",
    "                \"prompt_tokens\": 6023,\n",
    "                \"completion_tokens\": 108,\n",
    "                \"total_tokens\": 6131\n",
    "            },\n",
    "            {\n",
    "                \"prompt_tokens\": 6256,\n",
    "                \"completion_tokens\": 638,\n",
    "                \"total_tokens\": 6894\n",
    "            }\n",
    "        ]\n",
    "\n",
    "costs = calculate_token_cost(token_usage_list=token_usage,input_rate=0.59,output_rate=0.79)\n",
    "print(costs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jinqi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
