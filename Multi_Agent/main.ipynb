{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 取得專案根目錄 (lib 的父目錄)\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# 將 lib 加入 Python 模組搜尋路徑\n",
    "sys.path.append(os.path.join(root_dir, \"lib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "from tools import search\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# 683 tokens\n",
    "planner_system_prompt = \"\"\"You are an AI Planner Agent designed to handle complex questions using a **decomposition-first strategy**.\n",
    "\n",
    "## Your Goal:\n",
    "Make decisions on how to break down complex questions or provide final answers when appropriate.\n",
    "\n",
    "---\n",
    "\n",
    "## Available Actions\n",
    "\n",
    "1. **question_decompose**  \n",
    "   - Use this when a question is **too broad, multi-faceted, abstract, or contains multiple sub-goals**.\n",
    "   - You may decompose **one or more existing questions** — including the original question or any previous sub-questions.\n",
    "   - **Important:** You can **only decompose questions that already exist** in the conversation context. Do not introduce new parent questions that have not appeared before.\n",
    "   - Each decomposition must map a `parent_q` to a list of `sub_questions`.\n",
    "   - You must provide a global `reason` explaining **why** decomposition is needed.\n",
    "   - Format:\n",
    "     ```json\n",
    "     {\n",
    "       \"reason\": \"These questions require finer-grained analysis.\",\n",
    "       \"mapping\": [\n",
    "         {\n",
    "           \"parent_q\": \"How does climate change affect agriculture?\",\n",
    "           \"sub_questions\": [\n",
    "             \"How does temperature rise affect crop yield?\",\n",
    "             \"How does drought impact livestock?\"\n",
    "           ]\n",
    "         },\n",
    "         {\n",
    "           \"parent_q\": \"What are the social impacts of climate change?\",\n",
    "           \"sub_questions\": [\n",
    "             \"How does climate change affect migration patterns?\",\n",
    "             \"What mental health issues are linked to climate change?\"\n",
    "           ]\n",
    "         }\n",
    "       ]\n",
    "     }\n",
    "     ```\n",
    "\n",
    "2. **final_answer**\n",
    "   - Use this only when you believe the question can be directly answered, either based on prior knowledge or based on the `<sub-answers>` returned by previous decompositions.\n",
    "   - Provide a `reason` explaining **why** a final answer can now be given.\n",
    "   - Your answer should be clear, comprehensive, and informative—sufficient in length to convey key insights.\n",
    "\n",
    "---\n",
    "\n",
    "## Observation Rules\n",
    "\n",
    "- You may receive a block like:\n",
    "  ```\n",
    "  <sub-answers>\n",
    "  [ ... structured list of sub-questions and their summarized answers ... ]\n",
    "  </sub-answers>\n",
    "  ```\n",
    "  This means earlier decomposed questions have been resolved. You should:\n",
    "  - Consider whether these provide enough context to synthesize a final answer.\n",
    "  - Or decide whether **further decomposition** is needed for any sub-question.\n",
    "\n",
    "---\n",
    "\n",
    "## Reasoning Format\n",
    "\n",
    "For every action:\n",
    "1. First, **explain your reasoning** clearly in natural language.\n",
    "2. Then, **invoke one of the tools** using structured output (function call).\n",
    "\n",
    "---\n",
    "\n",
    "## Examples\n",
    "\n",
    "### Decomposing Questions\n",
    "\n",
    "'''\n",
    "The original question is too broad, spanning environmental, economic, and social dimensions. I will decompose it into manageable parts for better analysis.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"reason\": \"The question requires analysis across multiple domains.\",\n",
    "  \"mapping\": [\n",
    "    {\n",
    "      \"parent_q\": \"How can we reduce global carbon emissions?\",\n",
    "      \"sub_questions\": [\n",
    "        \"What are the biggest sources of carbon emissions?\",\n",
    "        \"What policies have proven effective in reducing emissions?\",\n",
    "        \"What role does individual behavior play in emission reduction?\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "'''\n",
    "\n",
    "### Providing a Final Answer\n",
    "\n",
    "'''\n",
    "I have received sufficient sub-answers, and they together form a complete picture. I can now provide a final answer to the original question.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"reason\": \"The provided sub-answers cover all aspects of the original question.\",\n",
    "}\n",
    "```\n",
    "'''\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MindSearch:\n",
    "    def __init__(self, llm, system_prompt, max_turns=9, debug_log=\"mindsearch_debug.log\", summary_json=\"mindsearch_summary.json\"):\n",
    "        self.planner = Agent(llm=llm)\n",
    "        self.searcher = Agent(llm=llm)\n",
    "        self.max_turns = max_turns\n",
    "        self.planner_conversation = [\n",
    "            {\"role\": \"developer\", \"content\": system_prompt}]\n",
    "        self.conversation_log = []  # 用於詳細記錄每一條訊息，不做傳入模型用\n",
    "        self.total_tokens = []  # 用於詳細記錄每個resopnse的tokens數量\n",
    "        self.questions = []\n",
    "\n",
    "        # Setup detailed debug logging\n",
    "        logging.basicConfig(filename=debug_log, level=logging.DEBUG,\n",
    "                            format=\"%(asctime)s [%(levelname)s] %(message)s\", encoding=\"utf-8\")\n",
    "        logging.info(\"\\n=== New ReAct Execution Started ===\\n\")\n",
    "\n",
    "        # Summary log file\n",
    "        self.summary_json = summary_json\n",
    "\n",
    "        # Initialize JSON file if it doesn't exist\n",
    "        if not os.path.exists(self.summary_json):\n",
    "            with open(self.summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump([], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        self.tools = [{\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"question_decompose\",\n",
    "                \"description\": \"Decompose one or more complex questions into sub-questions with reasoning.\",\n",
    "                \"strict\": True,\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"reason\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Why the decomposition is needed\"\n",
    "                        },\n",
    "                        \"mapping\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"description\": \"List of parent questions and their sub-questions\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"parent_q\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"The original question being broken down\"\n",
    "                                    },\n",
    "                                    \"sub_questions\": {\n",
    "                                        \"type\": \"array\",\n",
    "                                        \"items\": {\"type\": \"string\"},\n",
    "                                        \"description\": \"List of sub-questions derived from the parent question\"\n",
    "                                    }\n",
    "                                },\n",
    "                                \"required\": [\"parent_q\", \"sub_questions\"],\n",
    "                                \"additionalProperties\": False\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"reason\", \"mapping\"],\n",
    "                    \"additionalProperties\": False\n",
    "                }\n",
    "            }\n",
    "        }, {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"final_answer\",\n",
    "                \"description\": (\n",
    "                    \"Based on the current information and your confidence, explain why a final answer can be generated.\"\n",
    "                ),\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"reason\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Explain why you now have enough information to provide a final answer.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"reason\"],\n",
    "                    \"additionalProperties\": False\n",
    "                },\n",
    "                \"strict\": True\n",
    "            }\n",
    "        }]\n",
    "\n",
    "    def _normalize(self, q: str) -> str:\n",
    "        return q.lower().strip().rstrip(\"?。！？\")\n",
    "\n",
    "    def handle_tool_call(self, tool_call):\n",
    "        \"\"\"Executes the function requested by OpenAI's function calling system.\"\"\"\n",
    "        function_name = tool_call.function.name\n",
    "\n",
    "        try:\n",
    "            arguments = json.loads(tool_call.function.arguments)\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.warning(f\"JSON Decode Error in tool_call arguments: {e}\")\n",
    "            return \"retry\", None\n",
    "\n",
    "        logging.info(f\"Tool called: {function_name} with args: {arguments}\")\n",
    "\n",
    "        if function_name == \"question_decompose\":\n",
    "            mapping = arguments.get(\"mapping\")\n",
    "\n",
    "            valid_new_questions = []\n",
    "            sub_answers = []\n",
    "\n",
    "            for item in mapping:\n",
    "                parent_q = item.get(\"parent_q\")\n",
    "                sub_qs = item.get(\"sub_questions\", [])\n",
    "\n",
    "                if self._normalize(parent_q) not in [self._normalize(q) for q in self.questions]:\n",
    "                    logging.warning(f\"Parent question not found: {parent_q}\")\n",
    "                    continue\n",
    "\n",
    "                sub_answer = self.get_sub_answers(parent_q, sub_qs)\n",
    "                sub_answers.append(sub_answer)\n",
    "\n",
    "                valid_new_questions.extend(sub_qs)\n",
    "\n",
    "            if len(sub_answers) == 0:  # 只要有一個 parent_q 存在，sub_answers 就會有東西。反之，就會是空的。\n",
    "                logging.warning(\"No valid existed questions found.\")\n",
    "                return \"retry\", None\n",
    "\n",
    "            self.questions.extend(valid_new_questions)\n",
    "\n",
    "            return \"decompose\", sub_answers\n",
    "\n",
    "        elif function_name == \"search\":\n",
    "            query = arguments.get(\"query\")\n",
    "\n",
    "            logging.info(f\"Executing search for: {query}\")\n",
    "            return \"search\", search(query, max_results=5)\n",
    "\n",
    "        elif function_name == \"summary\":\n",
    "            summary = arguments.get(\"summary\")\n",
    "\n",
    "            logging.info(f\"summary: {summary}\")\n",
    "            return \"summary\", summary\n",
    "\n",
    "        elif function_name == \"final_answer\":\n",
    "            logging.info(\"Generating final answer...\")\n",
    "            return \"answer\", arguments.get(\"reason\")\n",
    "\n",
    "        else:\n",
    "            logging.warning(f\"Unknown function requested: {function_name}\")\n",
    "            return \"retry\", None\n",
    "\n",
    "    def get_sub_answers(self, question, sub_questions):\n",
    "        sub_answers = []\n",
    "        searcher_system_prompt = \"\"\"You are an AI assistant designed to answer sub-questions. You will be presented with original questions and sub-questions that you need to answer using a search engine.\n",
    "        Your task is to.\n",
    "        1. come up with the most appropriate query based on the original question and the sub-question.\n",
    "        2. summarize the search results to answer the sub-question based on the original question and the sub-question.\"\"\"\n",
    "\n",
    "        for sub_question in sub_questions:\n",
    "\n",
    "            searcher_conversation = [{\"role\": \"developer\", \"content\": searcher_system_prompt},\n",
    "                                     {\"role\": \"user\", \"content\": f\"The parent question: {question}, the sub-question: {sub_question}\"}]\n",
    "\n",
    "            tool = [\n",
    "                {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": \"search\",\n",
    "                        \"description\": \"Retrieve relevant web search results for a given query.\",\n",
    "                        \"strict\": True,\n",
    "                        \"parameters\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"reason\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                    \"description\": \"Reason why this search is needed.\"\n",
    "                                },\n",
    "                                \"query\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                    \"description\": \"Search query string.\"\n",
    "                                }\n",
    "                            },\n",
    "                            \"required\": [\n",
    "                                \"reason\",\n",
    "                                \"query\"\n",
    "                            ],\n",
    "                            \"additionalProperties\": False\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": \"summary\",\n",
    "                        \"description\": \"summary retrieved data to answer the sub-question.\",\n",
    "                        \"strict\": True,\n",
    "                        \"parameters\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"summary\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                    \"description\": \"summary of the retrieved data.\"\n",
    "                                }\n",
    "                            },\n",
    "                            \"required\": [\"summary\"],\n",
    "                            \"additionalProperties\": False\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "            # search\n",
    "            response, usage = self.searcher.generate_response(\n",
    "                conversations=searcher_conversation, tools=tool, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"search\"}})\n",
    "\n",
    "            self.total_tokens.append({\n",
    "                \"prompt_tokens\": usage.prompt_tokens,\n",
    "                \"completion_tokens\": usage.completion_tokens,\n",
    "                \"total_tokens\": usage.total_tokens\n",
    "            })\n",
    "\n",
    "            search_call = response.tool_calls[0]\n",
    "\n",
    "            search_result = self.handle_tool_call(search_call)[1]\n",
    "\n",
    "            assistant_response = {\"role\": \"assistant\",\n",
    "                                  \"tool_calls\": response.tool_calls}\n",
    "\n",
    "            tool_response = {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": search_call.id,\n",
    "                \"content\": str(search_result)\n",
    "            }\n",
    "\n",
    "            searcher_conversation.append(assistant_response)\n",
    "            searcher_conversation.append(tool_response)\n",
    "\n",
    "            # summary\n",
    "\n",
    "            response, usage = self.searcher.generate_response(\n",
    "                conversations=searcher_conversation, tools=tool, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"summary\"}})\n",
    "\n",
    "            self.total_tokens.append({\n",
    "                \"prompt_tokens\": usage.prompt_tokens,\n",
    "                \"completion_tokens\": usage.completion_tokens,\n",
    "                \"total_tokens\": usage.total_tokens\n",
    "            })\n",
    "\n",
    "            summary_call = response.tool_calls[0]\n",
    "            summary_result = self.handle_tool_call(summary_call)[1]\n",
    "\n",
    "            self.conversation_log.extend([\n",
    "                {\"agent\": \"searcher\", **msg} for msg in searcher_conversation\n",
    "            ])\n",
    "\n",
    "            assistant_response = {\"agent\": \"searcher\", \"role\": \"assistant\",\n",
    "                                  \"tool_calls\": response.tool_calls}\n",
    "\n",
    "            tool_response = {\n",
    "                \"agent\": \"searcher\",\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": summary_call.id,\n",
    "                \"content\": str(summary_result)\n",
    "            }\n",
    "\n",
    "            self.conversation_log.append(assistant_response)\n",
    "            self.conversation_log.append(tool_response)\n",
    "\n",
    "            sub_answers.append(\n",
    "                {\"sub_q\": sub_question, \"answer\": summary_result})\n",
    "\n",
    "        return {\n",
    "            \"parent_q\": question,\n",
    "            \"sub_answers\": sub_answers\n",
    "        }\n",
    "\n",
    "    def final_answer(self):\n",
    "        logging.info(\"Generating final answer...\")\n",
    "\n",
    "        response, usage = self.planner.generate_response(\n",
    "            self.planner_conversation, tools=self.tools, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"final_answer\"}})\n",
    "\n",
    "        self.total_tokens.append({\n",
    "            \"prompt_tokens\": usage.prompt_tokens,\n",
    "            \"completion_tokens\": usage.completion_tokens,\n",
    "            \"total_tokens\": usage.total_tokens\n",
    "        })\n",
    "\n",
    "        assistant_response = {\"role\": \"assistant\",\n",
    "                              \"tool_calls\": response.tool_calls}\n",
    "        \n",
    "        self.planner_conversation.append(assistant_response)\n",
    "        self.conversation_log.append(assistant_response)\n",
    "\n",
    "        tool_call = response.tool_calls[0]\n",
    "        state, feedback = self.handle_tool_call(tool_call)\n",
    "\n",
    "        tool_response = {\n",
    "            \"agent\": \"planner\",\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "            \"content\": feedback,\n",
    "        }\n",
    "        self.planner_conversation.append(tool_response)\n",
    "        self.conversation_log.append(tool_response)\n",
    "\n",
    "    def _save_summary(self):\n",
    "        \"\"\"Saves the ReAct session to JSON with ordered retrieved data.\"\"\"\n",
    "        # If file is empty or invalid, initialize as empty list\n",
    "        if not os.path.exists(self.summary_json) or os.stat(self.summary_json).st_size == 0:\n",
    "            data = []\n",
    "        else:\n",
    "            try:\n",
    "                with open(self.summary_json, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)  # Load existing data\n",
    "            except json.JSONDecodeError:\n",
    "                logging.warning(\"JSON file is corrupted. Resetting to empty.\")\n",
    "                data = []  # Reset JSON if it's corrupted\n",
    "\n",
    "        # 把 message 中任何非可序列化的欄位轉換為字串或 dict\n",
    "        serializable_log = []\n",
    "\n",
    "        for msg in self.conversation_log:\n",
    "            if isinstance(msg, dict):\n",
    "                msg_copy = msg.copy()\n",
    "                if \"tool_calls\" in msg_copy:\n",
    "                    msg_copy[\"tool_calls\"] = [tc.model_dump() if hasattr(tc, \"model_dump\") else str(tc)\n",
    "                                              for tc in msg_copy[\"tool_calls\"]]\n",
    "                serializable_log.append(msg_copy)\n",
    "            else:\n",
    "                serializable_log.append(str(msg))\n",
    "\n",
    "        session_summary = {\n",
    "            \"question\": self.conversation_log[1][\"content\"].replace(\"Question: \", \"\"),\n",
    "            \"conversations\": serializable_log,\n",
    "            \"token_usage\": self.total_tokens\n",
    "        }\n",
    "\n",
    "        data.append(session_summary)\n",
    "\n",
    "        with open(self.summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False,\n",
    "                      indent=4)  # Save updated data\n",
    "\n",
    "    def run(self, question):\n",
    "        logging.info(f\"Starting new session with question: {question}\")\n",
    "\n",
    "        user_question = {\"role\": \"user\", \"content\": f\"Question: {question}\"}\n",
    "\n",
    "        self.planner_conversation.append(user_question)\n",
    "\n",
    "        self.conversation_log = [{\"agent\": \"planner\", **msg}\n",
    "                                 for msg in self.planner_conversation]\n",
    "\n",
    "        self.questions.append(question)\n",
    "\n",
    "        turn = 1\n",
    "\n",
    "        while turn <= self.max_turns:\n",
    "            logging.info(f\"Turn {turn}: Planner's action.\")\n",
    "\n",
    "            # 1. Generate Thought + Action\n",
    "            response, usage = self.planner.generate_response(\n",
    "                self.planner_conversation, tools=self.tools, tool_choice=\"required\")\n",
    "            logging.info(f\"LLM Response:\\n{response}\")\n",
    "\n",
    "            self.total_tokens.append({\n",
    "                \"prompt_tokens\": usage.prompt_tokens,\n",
    "                \"completion_tokens\": usage.completion_tokens,\n",
    "                \"total_tokens\": usage.total_tokens\n",
    "            })\n",
    "\n",
    "            # 2. Check if the LLM requested a function call\n",
    "            if response.tool_calls:\n",
    "                assistant_response = {\"agent\": \"planner\", \"role\": \"assistant\",\n",
    "                                      \"tool_calls\": response.tool_calls}\n",
    "\n",
    "                self.planner_conversation.append(assistant_response)\n",
    "                self.conversation_log.append(assistant_response)\n",
    "\n",
    "                for tool_call in response.tool_calls:\n",
    "                    state, feedback = self.handle_tool_call(tool_call)\n",
    "\n",
    "                    if state == \"retry\":\n",
    "                        logging.warning(f\"Retrying Turn {turn}...\")\n",
    "                        turn -= 1\n",
    "                        continue\n",
    "\n",
    "                    if state == \"decompose\":\n",
    "                        tool_response = {\n",
    "                            \"agent\": \"planner\",\n",
    "                            \"role\": \"tool\",\n",
    "                            \"tool_call_id\": tool_call.id,\n",
    "                            \"content\": json.dumps(feedback, ensure_ascii=False),\n",
    "                        }\n",
    "                        self.planner_conversation.append(tool_response)\n",
    "                        self.conversation_log.append(tool_response)\n",
    "\n",
    "                        logging.info(f\"sub-answers: {str(feedback)}\")\n",
    "\n",
    "                    elif state == \"answer\":\n",
    "                        tool_response = {\n",
    "                            \"agent\": \"planner\",\n",
    "                            \"role\": \"tool\",\n",
    "                            \"tool_call_id\": tool_call.id,\n",
    "                            \"content\": feedback,\n",
    "                        }\n",
    "                        self.planner_conversation.append(tool_response)\n",
    "                        self.conversation_log.append(tool_response)\n",
    "\n",
    "                        response, usage = self.planner.generate_response(\n",
    "                            self.planner_conversation, tools=self.tools, tool_choice=\"none\")\n",
    "                        \n",
    "                        logging.info(f\"LLM Response:\\n{response.content}\")\n",
    "                        \n",
    "                        self.total_tokens.append({\"prompt_tokens\": usage.prompt_tokens,\n",
    "                                                  \"completion_tokens\": usage.completion_tokens,\n",
    "                                                  \"total_tokens\": usage.total_tokens})\n",
    "                        \n",
    "                        self.conversation_log.append(\n",
    "                            {\"agent\": \"planner\", \"role\": \"assistant\", \"content\": response.content})\n",
    "                        \n",
    "                        self._save_summary()\n",
    "                        logging.info(\"Final Answer Reached.\")\n",
    "                        return response.content\n",
    "\n",
    "            turn += 1\n",
    "\n",
    "        logging.warning(\"Max turns reached. No definitive answer found.\")\n",
    "        self.final_answer()\n",
    "\n",
    "        response, usage = self.planner.generate_response(\n",
    "            self.planner_conversation, tools=self.tools, tool_choice=\"none\")\n",
    "\n",
    "        self.total_tokens.append({\"prompt_tokens\": usage.prompt_tokens,\n",
    "                                  \"completion_tokens\": usage.completion_tokens,\n",
    "                                  \"total_tokens\": usage.total_tokens})\n",
    "\n",
    "        self.conversation_log.append(\n",
    "            {\"agent\": \"planner\", \"role\": \"assistant\", \"content\": response.content})\n",
    "\n",
    "        self._save_summary()\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing https://www.verywellmind.com/the-psychology-of-racism-5070459: HTTPSConnectionPool(host='www.verywellmind.com', port=443): Read timed out. (read timeout=5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Racism is a complex phenomenon influenced by a multitude of factors across historical, psychological, cultural, social, and economic dimensions:\\n\\n1. **Historical Factors:** Racism has roots in historical constructs like racial categorization, segregation, and systemic inequalities that have persisted over time. These constructs have historically entrenched power imbalances, creating long-lasting impacts on racial dynamics.\\n\\n2. **Psychological Factors:** Cognitive biases, such as stereotyping and ingrained automatic preferences for one's in-group over out-groups, contribute to prejudices. Additionally, cultural influences and media representations significantly shape these biases, reinforcing racist attitudes.\\n\\n3. **Cultural Aspects:** Persistent racist cultural narratives and biases ensure the longevity of racism. Cultural practices emphasize racial grouping and segregation, while media often perpetuates stereotypes and normalizes dominance of certain racial groups over others.\\n\\n4. **Social Dynamics:** The social organization of racial groups, reinforced by segregated communities and social norms, plays a significant role. While some social norms have evolved to discourage overt racism, underlying attitudes can remain, influenced by media and societal power structures.\\n\\n5. **Economic Conditions:** Economic disparities often perpetuate racial divisions, with historical and systemic policies contributing to such inequality. Economic disadvantage leads to stereotypes of minorities as economically burdensome, further fueling discrimination.\\n\\nUnderstanding these multifaceted contributions provides a more comprehensive picture of how racism is perpetuated and sustained within societies.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planner_system_prompt\n",
    "ms = MindSearch(llm=\"gpt-4o\", system_prompt=planner_system_prompt, max_turns=3)\n",
    "ms.run(\"what leads to racism?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_token_cost(token_usage_list, input_rate=2.5, output_rate=10, num_q=500):\n",
    "    \"\"\"\n",
    "    計算總成本（以新台幣計算），根據 token 使用量。\n",
    "\n",
    "    Args:\n",
    "        token_usage_list (list of dict): 每個 dict 包含 prompt_tokens, completion_tokens\n",
    "        input_rate (float): 輸入成本，單位為每 1M token 的新台幣（預設 2.5）\n",
    "        output_rate (float): 輸出成本，單位為每 1M token 的新台幣（預設 10）\n",
    "        num_q (int): 問多少道問題\n",
    "\n",
    "    Returns:\n",
    "        dict: 包含輸入、輸出、總成本與總 token\n",
    "    \"\"\"\n",
    "\n",
    "    total_prompt = sum(x[\"prompt_tokens\"] for x in token_usage_list)\n",
    "    total_completion = sum(x[\"completion_tokens\"] for x in token_usage_list)\n",
    "\n",
    "    input_cost = total_prompt * num_q * input_rate / 1000000\n",
    "    output_cost = total_completion * num_q * output_rate / 1000000\n",
    "    total_cost = input_cost + output_cost\n",
    "\n",
    "    return {\n",
    "        \"total_prompt_tokens\": total_prompt,\n",
    "        \"total_completion_tokens\": total_completion,\n",
    "        \"input_cost_ntd\": round(input_cost, 2),\n",
    "        \"output_cost_ntd\": round(output_cost, 2),\n",
    "        \"total_cost_ntd\": round(total_cost, 2)\n",
    "    }\n",
    "\n",
    "\n",
    "token_usage = [\n",
    "    {\n",
    "        \"prompt_tokens\": 877,\n",
    "        \"completion_tokens\": 90,\n",
    "        \"total_tokens\": 967\n",
    "    },\n",
    "    {\n",
    "        \"prompt_tokens\": 204,\n",
    "        \"completion_tokens\": 34,\n",
    "        \"total_tokens\": 238\n",
    "    },\n",
    "    {\n",
    "        \"prompt_tokens\": 1560,\n",
    "        \"completion_tokens\": 110,\n",
    "        \"total_tokens\": 1670\n",
    "    },\n",
    "    {\n",
    "        \"prompt_tokens\": 203,\n",
    "        \"completion_tokens\": 35,\n",
    "        \"total_tokens\": 238\n",
    "    },\n",
    "    {\n",
    "        \"prompt_tokens\": 7280,\n",
    "        \"completion_tokens\": 137,\n",
    "        \"total_tokens\": 7417\n",
    "    },\n",
    "    {\n",
    "        \"prompt_tokens\": 202,\n",
    "        \"completion_tokens\": 25,\n",
    "        \"total_tokens\": 227\n",
    "    },\n",
    "    {\n",
    "        \"prompt_tokens\": 10954,\n",
    "        \"completion_tokens\": 251,\n",
    "        \"total_tokens\": 11205\n",
    "    },\n",
    "    {\n",
    "        \"prompt_tokens\": 205,\n",
    "        \"completion_tokens\": 26,\n",
    "        \"total_tokens\": 231\n",
    "    },\n",
    "    {\n",
    "        \"prompt_tokens\": 6639,\n",
    "        \"completion_tokens\": 154,\n",
    "        \"total_tokens\": 6793\n",
    "    },\n",
    "    {\n",
    "        \"prompt_tokens\": 1698,\n",
    "        \"completion_tokens\": 171,\n",
    "        \"total_tokens\": 1869\n",
    "    }\n",
    "]\n",
    "\n",
    "costs = calculate_token_cost(token_usage)\n",
    "print(costs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jinqi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
