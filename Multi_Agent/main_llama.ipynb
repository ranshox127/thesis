{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 取得專案根目錄 (lib 的父目錄)\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# 將 lib 加入 Python 模組搜尋路徑\n",
    "sys.path.append(os.path.join(root_dir, \"lib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_llama import Agent_llama\n",
    "from tools import search\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# 683 tokens\n",
    "planner_system_prompt = \"\"\"You are an AI Planner Agent designed to handle complex questions using a **decomposition-first strategy**.\n",
    "\n",
    "## Your Goal:\n",
    "Make decisions on how to break down complex questions or provide final answers when appropriate.\n",
    "\n",
    "---\n",
    "\n",
    "## Available Actions\n",
    "\n",
    "1. **question_decompose**  \n",
    "   - Use this when a question is **too broad, multi-faceted, abstract, or contains multiple sub-goals**.\n",
    "   - You may decompose **one or more existing questions** — including the original question or any previous sub-questions.\n",
    "   - **Important:** You can **only decompose questions that already exist** in the conversation context. Do not introduce new parent questions that have not appeared before.\n",
    "   - Each decomposition must map a `parent_q` to a list of `sub_questions`.\n",
    "   - You must provide a global `reason` explaining **why** decomposition is needed.\n",
    "   - Format:\n",
    "     ```json\n",
    "     {\n",
    "       \"reason\": \"These questions require finer-grained analysis.\",\n",
    "       \"mapping\": [\n",
    "         {\n",
    "           \"parent_q\": \"How does climate change affect agriculture?\",\n",
    "           \"sub_questions\": [\n",
    "             \"How does temperature rise affect crop yield?\",\n",
    "             \"How does drought impact livestock?\"\n",
    "           ]\n",
    "         },\n",
    "         {\n",
    "           \"parent_q\": \"What are the social impacts of climate change?\",\n",
    "           \"sub_questions\": [\n",
    "             \"How does climate change affect migration patterns?\",\n",
    "             \"What mental health issues are linked to climate change?\"\n",
    "           ]\n",
    "         }\n",
    "       ]\n",
    "     }\n",
    "     ```\n",
    "\n",
    "2. **final_answer**\n",
    "   - Use this only when you believe the question can be directly answered, either based on prior knowledge or based on the `<sub-answers>` returned by previous decompositions.\n",
    "   - Provide a `reason` explaining **why** a final answer can now be given.\n",
    "   - Your answer should be clear, comprehensive, and informative—sufficient in length to convey key insights.\n",
    "   \n",
    "   Example:\n",
    "   ```json\n",
    "    {\n",
    "      \"reason\": \"The sub-questions cover key social dimensions — lifestyle, geography, and inequality — and their annotations provide sufficient insight.\",\n",
    "    }\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "## Observation Rules\n",
    "\n",
    "- You may receive a block like:\n",
    "  ```\n",
    "  <sub-answers>\n",
    "  [ ... structured list of sub-questions and their summarized answers ... ]\n",
    "  </sub-answers>\n",
    "  ```\n",
    "  This means earlier decomposed questions have been resolved. You should:\n",
    "  - Consider whether these provide enough context to synthesize a final answer.\n",
    "  - Or decide whether **further decomposition** is needed for any sub-question.\n",
    "\n",
    "---\n",
    "\n",
    "## Reasoning Format\n",
    "\n",
    "For every action:\n",
    "1. First, **explain your reasoning** clearly in natural language.\n",
    "2. Then, **invoke one of the tools** using structured output (function call).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MindSearch:\n",
    "    def __init__(self, llm, system_prompt, max_turns=9, debug_log=\"llama_mindsearch_debug.log\", summary_json=\"llama_mindsearch_summary.json\"):\n",
    "        self.planner = Agent_llama(llm=llm)\n",
    "        self.searcher = Agent_llama(llm=llm)\n",
    "        self.max_turns = max_turns\n",
    "        self.planner_conversation = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt}]\n",
    "        self.conversation_log = []  # 用於詳細記錄每一條訊息，不做傳入模型用\n",
    "        self.questions = []\n",
    "\n",
    "        # Setup detailed debug logging\n",
    "        logging.basicConfig(filename=debug_log, level=logging.DEBUG,\n",
    "                            format=\"%(asctime)s [%(levelname)s] %(message)s\", encoding=\"utf-8\")\n",
    "        logging.info(\"\\n=== New mindsearch Execution Started ===\\n\")\n",
    "\n",
    "        # Summary log file\n",
    "        self.summary_json = summary_json\n",
    "\n",
    "        # Initialize JSON file if it doesn't exist\n",
    "        if not os.path.exists(self.summary_json):\n",
    "            with open(self.summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump([], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def _normalize(self, q: str) -> str:\n",
    "        return q.lower().strip().rstrip(\"?。！？\")\n",
    "\n",
    "    def handle_tool_call(self, tool_call):\n",
    "        \"\"\"Executes the function requested by OpenAI's function calling system.\"\"\"\n",
    "        function_name = tool_call.tool_name\n",
    "\n",
    "        if function_name == \"search\":\n",
    "            query = tool_call.query\n",
    "\n",
    "            logging.info(f\"Executing search for: {query}\")\n",
    "            return \"search\", search(query, max_results=5)\n",
    "\n",
    "        elif function_name == \"summary\":\n",
    "            summary = tool_call.summary\n",
    "\n",
    "            logging.info(f\"summary: {summary}\")\n",
    "            return \"summary\", summary\n",
    "\n",
    "        try:\n",
    "            arguments = json.loads(tool_call.tool_parameters)\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.warning(f\"JSON Decode Error in tool_call arguments: {e}\")\n",
    "            return \"retry\", None\n",
    "\n",
    "        logging.info(f\"Tool called: {function_name} with args: {arguments}\")\n",
    "\n",
    "        if function_name == \"question_decompose\":\n",
    "            mapping = arguments.get(\"mapping\")\n",
    "\n",
    "            valid_new_questions = []\n",
    "            sub_answers = []\n",
    "\n",
    "            for item in mapping:\n",
    "                parent_q = item.get(\"parent_q\")\n",
    "                sub_qs = item.get(\"sub_questions\", [])\n",
    "\n",
    "                if self._normalize(parent_q) not in [self._normalize(q) for q in self.questions]:\n",
    "                    logging.warning(f\"Parent question not found: {parent_q}\")\n",
    "                    continue\n",
    "\n",
    "                sub_answer = self.get_sub_answers(parent_q, sub_qs)\n",
    "                sub_answers.append(sub_answer)\n",
    "\n",
    "                valid_new_questions.extend(sub_qs)\n",
    "\n",
    "            if len(sub_answers) == 0:  # 只要有一個 parent_q 存在，sub_answers 就會有東西。反之，就會是空的。\n",
    "                logging.warning(\"No valid existed questions found.\")\n",
    "                return \"retry\", None\n",
    "\n",
    "            self.questions.extend(valid_new_questions)\n",
    "\n",
    "            return \"decompose\", sub_answers\n",
    "\n",
    "        elif function_name == \"final_answer\":\n",
    "            logging.info(\"Generating final answer...\")\n",
    "            return \"answer\", arguments.get(\"reason\")\n",
    "\n",
    "        else:\n",
    "            logging.warning(f\"Unknown function requested: {function_name}\")\n",
    "            return \"retry\", None\n",
    "\n",
    "    def get_sub_answers(self, question, sub_questions):\n",
    "        sub_answers = []\n",
    "        searcher_system_prompt = \"\"\"You are an AI assistant designed to answer sub-questions. You will be presented with original questions and sub-questions that you need to answer using a search engine.\n",
    "        Your task is to.\n",
    "        1. come up with the most appropriate query based on the original question and the sub-question.\n",
    "        2. summarize the search results to answer the sub-question based on the original question and the sub-question.\n",
    "        \n",
    "        Use the available tools:\n",
    "        - `search(query, reason)`: to retrieve relevant information\n",
    "          Example:\n",
    "          ```json\n",
    "          {\n",
    "            \"reason\": \"To answer the question about dark energy, I need to find out who discovered it.\",\n",
    "            \"query\": \"Who discovered dark energy?\"\n",
    "          }\n",
    "           ```\n",
    "        - `summary(text)`: to return your synthesized summary\n",
    "          Example:\n",
    "          ```json\n",
    "          {\n",
    "            \"summary\": \"Dark energy was discovered by astronomer Edwin Hubble in 1929.\"\n",
    "          }\n",
    "          ```\n",
    "        \"\"\"\n",
    "\n",
    "        for sub_question in sub_questions:\n",
    "\n",
    "            searcher_conversation = [{\"role\": \"system\", \"content\": searcher_system_prompt},\n",
    "                                     {\"role\": \"user\", \"content\": f\"The parent question: {question}, the sub-question: {sub_question}\"}]\n",
    "\n",
    "            # search\n",
    "            response = self.searcher.generate_response(\n",
    "                conversations=searcher_conversation, action=\"Search\")\n",
    "\n",
    "            search_result = self.handle_tool_call(response)[1]\n",
    "\n",
    "            assistant_response = {\n",
    "                \"role\": \"assistant\", \"content\": f\"Tool's name:{response.tool_name}\\nreason:{response.reason}\\nquery:{response.query}\"}\n",
    "            searcher_conversation.append(assistant_response)\n",
    "\n",
    "            tool_response = {\"role\": \"user\", \"content\": str(search_result)}\n",
    "            searcher_conversation.append(tool_response)\n",
    "\n",
    "            # summary\n",
    "\n",
    "            response = self.searcher.generate_response(\n",
    "                conversations=searcher_conversation, action=\"Summary\")\n",
    "\n",
    "            summary_result = self.handle_tool_call(response)[1]\n",
    "\n",
    "            self.conversation_log.extend([\n",
    "                {\"agent\": \"searcher\", **msg} for msg in searcher_conversation\n",
    "            ])\n",
    "\n",
    "            assistant_response = {\"agent\": \"searcher\", \"role\": \"assistant\",\n",
    "                                  \"content\": f\"Tool's name:{response.tool_name}\"}\n",
    "            self.conversation_log.append(assistant_response)\n",
    "\n",
    "            tool_response = {\"agent\": \"searcher\",\n",
    "                             \"role\": \"user\", \"content\": summary_result}\n",
    "            self.conversation_log.append(tool_response)\n",
    "\n",
    "            sub_answers.append(\n",
    "                {\"sub_q\": sub_question, \"answer\": summary_result})\n",
    "\n",
    "        return {\n",
    "            \"parent_q\": question,\n",
    "            \"sub_answers\": sub_answers\n",
    "        }\n",
    "\n",
    "    def final_answer(self):\n",
    "        logging.info(\"Generating final answer...\")\n",
    "\n",
    "        final_answer_request = {\n",
    "            \"role\": \"user\", \"content\": \"Please organize the information you have gathered and write a complete and comprehensive answer to the original question.\"}\n",
    "        self.planner_conversation.append(final_answer_request)\n",
    "        self.conversation_log.append(\n",
    "            {**final_answer_request, \"agent\": \"planner\"})\n",
    "\n",
    "        response = self.planner.generate_response(\n",
    "            self.planner_conversation, action=\"FA\")\n",
    "\n",
    "        return response.final_answer\n",
    "\n",
    "    def _save_summary(self):\n",
    "        \"\"\"Saves the ReAct session to JSON with ordered retrieved data.\"\"\"\n",
    "        # If file is empty or invalid, initialize as empty list\n",
    "        if not os.path.exists(self.summary_json) or os.stat(self.summary_json).st_size == 0:\n",
    "            data = []\n",
    "        else:\n",
    "            try:\n",
    "                with open(self.summary_json, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)  # Load existing data\n",
    "            except json.JSONDecodeError:\n",
    "                logging.warning(\"JSON file is corrupted. Resetting to empty.\")\n",
    "                data = []  # Reset JSON if it's corrupted\n",
    "\n",
    "        # 把 message 中任何非可序列化的欄位轉換為字串或 dict\n",
    "        serializable_log = []\n",
    "\n",
    "        for msg in self.conversation_log:\n",
    "            if isinstance(msg, dict):\n",
    "                msg_copy = msg.copy()\n",
    "                if \"tool_calls\" in msg_copy:\n",
    "                    msg_copy[\"tool_calls\"] = [tc.model_dump() if hasattr(tc, \"model_dump\") else str(tc)\n",
    "                                              for tc in msg_copy[\"tool_calls\"]]\n",
    "                serializable_log.append(msg_copy)\n",
    "            else:\n",
    "                serializable_log.append(str(msg))\n",
    "\n",
    "        session_summary = {\n",
    "            \"question\": self.conversation_log[1][\"content\"].replace(\"Question: \", \"\"),\n",
    "            \"conversations\": serializable_log,\n",
    "        }\n",
    "\n",
    "        data.append(session_summary)\n",
    "\n",
    "        with open(self.summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False,\n",
    "                      indent=4)  # Save updated data\n",
    "\n",
    "    def run(self, question):\n",
    "        logging.info(f\"Starting new session with question: {question}\")\n",
    "\n",
    "        user_question = {\"role\": \"user\", \"content\": f\"Question: {question}\"}\n",
    "\n",
    "        self.planner_conversation.append(user_question)\n",
    "\n",
    "        self.conversation_log = [{\"agent\": \"planner\", **msg}\n",
    "                                 for msg in self.planner_conversation]\n",
    "\n",
    "        self.questions.append(question)\n",
    "\n",
    "        turn = 1\n",
    "\n",
    "        while turn <= self.max_turns:\n",
    "            logging.info(f\"Turn {turn}: Planner's action.\")\n",
    "\n",
    "            # 1. Generate Thought + Action\n",
    "            response = self.planner.generate_response(\n",
    "                self.planner_conversation, action=\"TC\")\n",
    "            logging.info(f\"LLM Response:\\n{response}\")\n",
    "\n",
    "            # 2. process agents function call\n",
    "\n",
    "            # 2.1 record the assistant\n",
    "            assistant_response = {\n",
    "                \"role\": \"assistant\", \"content\": f\"Tool's name:{response.tool_name}\\nTool's parameters:{response.tool_parameters}\"}\n",
    "            self.planner_conversation.append(assistant_response)\n",
    "            self.conversation_log.append(\n",
    "                {**assistant_response, \"agent\": \"planner\"})\n",
    "\n",
    "            state, feedback = self.handle_tool_call(response)\n",
    "\n",
    "            if state == \"retry\":\n",
    "                logging.warning(f\"Retrying Turn {turn}...\")\n",
    "                turn -= 1\n",
    "                self.planner_conversation.pop()\n",
    "                self.conversation_log.pop()\n",
    "                continue\n",
    "\n",
    "            if state == \"decompose\":\n",
    "                tool_response = {\"role\": \"user\", \"content\": json.dumps(\n",
    "                    feedback, ensure_ascii=False)}\n",
    "                self.planner_conversation.append(tool_response)\n",
    "                self.conversation_log.append(\n",
    "                    {**tool_response, \"agent\": \"planner\"})\n",
    "\n",
    "                logging.info(f\"sub-answers: {str(feedback)}\")\n",
    "\n",
    "            elif state == \"answer\":\n",
    "                tool_response = {\n",
    "                    \"role\": \"user\", \"content\": \"Please organize the information you have gathered and write a complete and comprehensive answer to the original question.\"}\n",
    "                self.planner_conversation.append(tool_response)\n",
    "                self.conversation_log.append(\n",
    "                    {**tool_response, \"agent\": \"planner\"})\n",
    "\n",
    "                response = self.planner.generate_response(\n",
    "                    self.planner_conversation, action=\"FA\")\n",
    "\n",
    "                logging.info(f\"LLM Response:\\n{response.final_answer}\")\n",
    "\n",
    "                self.conversation_log.append(\n",
    "                    {\"agent\": \"planner\", \"role\": \"assistant\", \"content\": response.final_answer})\n",
    "\n",
    "                self._save_summary()\n",
    "                logging.info(\"Final Answer Reached.\")\n",
    "                return response.final_answer\n",
    "\n",
    "            turn += 1\n",
    "\n",
    "        logging.warning(\"Max turns reached. No definitive answer found.\")\n",
    "        final_answer = self.final_answer()\n",
    "\n",
    "        self.conversation_log.append(\n",
    "            {\"agent\": \"planner\", \"role\": \"assistant\", \"content\": final_answer})\n",
    "\n",
    "        self._save_summary()\n",
    "        return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Racism is a complex and deeply ingrained issue that arises from a combination of historical, social, economic, and cultural factors. Historically, the roots of racism can be traced back to the period of the Renaissance and Reformation, when Europeans began making judgments about people of darker pigmentation in Africa, Asia, and the Americas. The Enlightenment brought a secular or scientific theory of race, subdividing humans into three to five races, and the 18th and 19th centuries saw the growth of ideological racism in Europe and the United States. The 20th century saw the rise and fall of overtly racist regimes, with the most extreme example being Nazi Germany, and the Civil Rights movement in the United States succeeding in outlawing legalized racial segregation and discrimination in the 1960s. Today, discrimination by institutions and individuals against those perceived as racially different can persist and flourish under the illusion of non-racism. Social and economic conditions, such as poverty, unemployment, and lack of access to education and healthcare, can contribute to racism by creating an environment in which racist attitudes and behaviors can thrive. Additionally, discrimination in areas such as housing, employment, and the justice system can perpetuate racial disparities and reinforce racist stereotypes. Culture and education also play a significant role in shaping racial attitudes, with diversity, equity, and inclusion promoting a sense of belonging, improving performance, and enhancing social cohesion. Therefore, addressing racism requires a comprehensive approach that takes into account its historical, social, economic, and cultural dimensions, and involves efforts to promote diversity, equity, and inclusion, as well as to address the root causes of racial disparities and discrimination.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms = MindSearch(llm=\"llama 3.3 70B\", system_prompt=planner_system_prompt, max_turns=9)\n",
    "ms.run(\"what leads to racism?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jinqi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
