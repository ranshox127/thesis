{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a5424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "eval_prompt_p1 = \"\"\"You are asked to assess the quality of an AI assistant's answer to a user's question as an impartial judge. Since the type of answer you are evaluating is [Solve Professional Problem], you need to evaluate the answer in the following 5 criteria:\n",
    "1. Factuality: Whether the information provided is accurate and based on reliable facts and data.\n",
    "2. User Satisfaction: Whether the response meets the user's question and needs and provides a comprehensive and appropriate answer to the question.\n",
    "3. Clarity: Whether the response is clear and understandable, and whether it uses concise language and structure so that the user can easily understand it.\n",
    "4. Logical Coherence: Whether the response maintains overall consistency and logical coherence between different sections, avoiding self-contradiction.\n",
    "5. Completeness: Whether the response provides sufficient information and details to meet the user's needs, and whether it avoids omitting important aspects.\n",
    "6. Note that a longer answer is not always better, the answer that is concise and meets the above requirements is the best.\n",
    "\n",
    "We will provide you with the user's question, an 8-score reference answer, and answers from the AI assistant that needs your assessment. When starting your evaluation, you need to follow the reasoning steps below:\n",
    "1. Compare the AI assistant's answer with the reference answer, point out any shortcomings in the AI assistant's answer, and explain further.\n",
    "2. Evaluate the AI assistant's answer in terms of the different criteria, giving each criterion a score from 1 to 10 after the evaluation of each.\n",
    "3. Finally, combine the evaluations from each criterion and give the AI assistant's answer a composite score of 1 to 10.\n",
    "4. Your scoring needs to be as rigorous as possible and adhere to the following scoring rules: in general, the higher the quality of the model's answers, the higher the score.\n",
    "The two most important criteria are factual correctness and fulfillment of user needs, and the scores for these two dimensions dominate the final composite score.\n",
    "\n",
    "When the model answer has irrelevance to the question, or intrinsically factually incorrect, or generates harmful content, the total score should be 1 to 2;\n",
    "When the model answer has no serious errors and is largely harmless, but is of low quality and does not meet user requirements, the total score must be 3 to 4;\n",
    "When the model answer basically meets the user's needs but performs poorly on some criteria and is of medium quality, the total score can be 5 to 6;\n",
    "When the quality of the model response is similar to the reference answer and performs well in all criteria, the total score should be 7 to 8;\n",
    "A score of 9 to 10 can only be achieved if the model significantly exceeds the quality of the reference answer, adequately addresses the user's question and all the needs, and is close to a perfect score on all criteria. As an example, the reference answer would receive a score of 8.\n",
    "\n",
    "You need to evaluate and explain before you score. Your explanation of each criterion needs to be followed by the scoring. After that, at the end of your answer, return all of your scores in the following dictionary format, including the curly brackets, and make sure that your scores are integers:\n",
    "{'Dimension 1': scoring, 'Dimension 2': scoring, ... , 'Final Score': Score}, e.g. {'Factuality': 9, 'User Satisfaction': 6, ... , 'Final Score': 7}.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41002d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input file created: llama_ver_batch_evaluation_RQ3.jsonl\n",
      "Batch input file uploaded with ID: file_01jtzxks0jfwm8jnvgvp4t9g7b\n",
      "Batch created with ID: batch_01jtzxksbkeevbxsjcqx5ytc7n\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "def prepare_batch_evaluation(results_file=\"./result/final_experiment_results.jsonl\",\n",
    "                             output_batch_file=\"batch_evaluation.jsonl\"):\n",
    "    \"\"\"\n",
    "    æº–å‚™ Batch API è¼¸å…¥æª”æ¡ˆï¼Œä»¥è©•ä¼° final_experiment_results.jsonl ä¸­çš„ç­”æ¡ˆã€‚\n",
    "    ä½¿ç”¨ system == \"MindSearch\" å’Œ model == \"gpt-4o\" çš„ç­”æ¡ˆä½œç‚ºåƒè€ƒç­”æ¡ˆã€‚\n",
    "    è©•ä¼°æ‰€æœ‰å…¶ä»–æ¨¡å‹å’Œç³»çµ±çµ„åˆçš„ç­”æ¡ˆã€‚\n",
    "    \"\"\"\n",
    "    batch_requests = []\n",
    "    reference_answers = {}\n",
    "    data_to_evaluate = {}\n",
    "    \n",
    "    RQ3_llama_best_After_CD = \"final_RQ3_experiment_results.jsonl\"\n",
    "\n",
    "    # è¼‰å…¥åƒè€ƒç­”æ¡ˆ (MindSearch, gpt-4o)\n",
    "    with open(results_file, 'r') as f_ref:\n",
    "        for line in f_ref:\n",
    "            try:\n",
    "                record = json.loads(line.strip())\n",
    "                if record.get(\"system\") == \"MindSearch\" and record.get(\"model\") == \"gpt-4o\" and record.get(\"question_id\"):\n",
    "                    reference_answers[record[\"question_id\"]] = {\n",
    "                        \"answer\": record.get(\"answer\"),\n",
    "                        \"question\": record.get(\"question\")\n",
    "                    }\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding reference answers: {e}\")\n",
    "                continue\n",
    "\n",
    "    # è¼‰å…¥éœ€è¦è©•ä¼°çš„ç­”æ¡ˆ (æ‰€æœ‰å…¶ä»–çµ„åˆ)\n",
    "    with open(RQ3_llama_best_After_CD, 'r') as f_eval:\n",
    "        for line in f_eval:\n",
    "            try:\n",
    "                record = json.loads(line.strip())\n",
    "                question_id = record.get(\"question_id\")\n",
    "                system = record.get(\"system\")\n",
    "                model = record.get(\"model\")\n",
    "                answer = record.get(\"answer\")\n",
    "                question = record.get(\"question\")\n",
    "\n",
    "                if question_id and answer is not None and not (system == \"MindSearch\" and model == \"gpt-4o\") and question_id in reference_answers:\n",
    "                    if question_id not in data_to_evaluate:\n",
    "                        data_to_evaluate[question_id] = {}\n",
    "                    data_to_evaluate[question_id][f\"{system}-{model}\"] = answer\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding evaluation data: {e}\")\n",
    "                continue\n",
    "            \n",
    "    print(f\"Loaded {len(reference_answers)} reference answers.\")\n",
    "    print(f\"Attempting to evaluate answers for {len(data_to_evaluate)} question IDs.\")\n",
    "\n",
    "    evaluated_count = 0\n",
    "    skipped_due_to_no_reference = 0\n",
    "\n",
    "    # æº–å‚™ Batch API è«‹æ±‚\n",
    "    for question_id, answers_by_model in data_to_evaluate.items():\n",
    "        reference_info = reference_answers.get(question_id)\n",
    "        if reference_info:\n",
    "            reference_answer = reference_info[\"answer\"]\n",
    "            question = reference_info[\"question\"]\n",
    "            for model_system, evaluated_answer in answers_by_model.items():\n",
    "                try:\n",
    "                    system, model = model_system.split('-', 1)\n",
    "                    custom_id = f\"{question_id}-{system}-{model}\"\n",
    "\n",
    "                    eval_prompt_p2 = f\"\"\"Question: \"{question}\"\n",
    "<Reference Answer>\n",
    "{reference_answer}\n",
    "</Reference Answer>\n",
    "\n",
    "<AI assistant's answer>\n",
    "{evaluated_answer}\n",
    "</AI assistant's answer>\"\"\"\n",
    "\n",
    "                    batch_requests.append({\n",
    "                        \"custom_id\": custom_id,\n",
    "                        \"method\": \"POST\",\n",
    "                        \"url\": \"/v1/chat/completions\",\n",
    "                        \"body\": {\n",
    "                            \"model\": \"llama-3.3-70b-versatile\",\n",
    "                            \"messages\": [{\"role\": \"system\", \"content\": eval_prompt_p1},\n",
    "                                         {\"role\": \"user\", \"content\": eval_prompt_p2}]\n",
    "                        }\n",
    "                    })\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error splitting model_system '{model_system}': {e}\")\n",
    "                    continue # è·³éé€™å€‹æœ‰å•é¡Œçš„ model_system\n",
    "                \n",
    "            evaluated_count += len(answers_by_model)\n",
    "            \n",
    "        else:\n",
    "            skipped_due_to_no_reference += len(answers_by_model)\n",
    "            print(f\"Warning: No reference answer found for question_id '{question_id}'. Skipping evaluation for: {answers_by_model.keys()}\")\n",
    "    print(f\"Created {len(batch_requests)} evaluation requests.\")\n",
    "    print(f\"Skipped {skipped_due_to_no_reference} evaluations due to missing reference answers.\")\n",
    "\n",
    "    # å°‡è«‹æ±‚å¯«å…¥ Batch è¼¸å…¥æª”æ¡ˆ\n",
    "    with open(output_batch_file, 'w') as f:\n",
    "        for req in batch_requests:\n",
    "            f.write(json.dumps(req) + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "    output_batch_file = \"llama_ver_batch_evaluation_RQ3.jsonl\"\n",
    "    # prepare_batch_evaluation(output_batch_file=output_batch_file)\n",
    "    print(f\"Batch input file created: {output_batch_file}\")\n",
    "\n",
    "    # æ­¥é©Ÿ 2: ä¸Šå‚³ Batch è¼¸å…¥æª”æ¡ˆ\n",
    "    try:\n",
    "        with open(output_batch_file, \"rb\") as f:\n",
    "            batch_input_file = client.files.create(\n",
    "                file=f,\n",
    "                purpose=\"batch\"\n",
    "            )\n",
    "        print(f\"Batch input file uploaded with ID: {batch_input_file.id}\")\n",
    "        input_file_id = batch_input_file.id\n",
    "\n",
    "        # æ­¥é©Ÿ 3: å‰µå»º Batch\n",
    "        batch = client.batches.create(\n",
    "            input_file_id=input_file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\"description\": \"Batch evaluation of different model/system combinations\"}\n",
    "        )\n",
    "        print(f\"Batch created with ID: {batch.id}\")\n",
    "\n",
    "        # **å¾ŒçºŒæ­¥é©Ÿï¼šè¼ªè©¢ç‹€æ…‹ã€æª¢ç´¢çµæœã€è§£æå’Œè¨˜éŒ„çµæœ**\n",
    "        # æ‚¨éœ€è¦å¯¦ç¾é€™äº›æ­¥é©Ÿï¼Œå°±åƒä¹‹å‰çš„æµç¨‹ä¸€æ¨£ã€‚\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Batch API interaction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d057618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchRetrieveResponse(id='batch_01jtzxksbkeevbxsjcqx5ytc7n', completion_window='24h', created_at=1746975450, endpoint='/v1/chat/completions', input_file_id='file_01jtzxks0jfwm8jnvgvp4t9g7b', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1746975470, error_file_id=None, errors=None, expired_at=None, expires_at=1747061850, failed_at=None, finalizing_at=1746975470, in_progress_at=1746975454, metadata={'description': 'Batch evaluation of different model/system combinations'}, output_file_id='file_01jtzxmckhew9ve80v2fqgbswv', request_counts=RequestCounts(completed=500, failed=0, total=500))\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "client = Groq()\n",
    "\n",
    "batch = client.batches.retrieve(\"batch_01jtzxksbkeevbxsjcqx5ytc7n\")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0df94699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "client = Groq()\n",
    "\n",
    "file_response = client.files.content(\"file_01jtzxmckhew9ve80v2fqgbswv\")\n",
    "output_filename = \"llama_ver_RQ3_evaluation_results.jsonl\"\n",
    "file_response.write_to_file(output_filename)\n",
    "# with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
    "#     outfile.write(file_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b43a917d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å®Œæˆï¼Œå…±è™•ç† 500 ç­†è³‡æ–™ã€‚çµæœå·²å„²å­˜è‡³ llama_ver_RQ3_evaluation_results.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_file = 'llama_ver_RQ3_evaluation_results.jsonl'\n",
    "output_file = 'llama_ver_RQ3_evaluation_results.jsonl'\n",
    "\n",
    "cleaned_data = []\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            entry = json.loads(line)\n",
    "            custom_id = entry.get(\"custom_id\")\n",
    "            content = (\n",
    "                entry.get(\"response\", {})\n",
    "                     .get(\"body\", {})\n",
    "                     .get(\"choices\", [{}])[0]\n",
    "                     .get(\"message\", {})\n",
    "                     .get(\"content\", \"\")\n",
    "            )\n",
    "            cleaned_data.append({\n",
    "                \"custom_id\": custom_id,\n",
    "                \"content\": content\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è™•ç†å¤±æ•—: {e}\")\n",
    "            continue\n",
    "\n",
    "# âœ… å¯é¸ï¼šå¯«å…¥æ¸…ç†å¾Œçš„ .jsonl æª”æ¡ˆ\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for item in cleaned_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"âœ… å®Œæˆï¼Œå…±è™•ç† {len(cleaned_data)} ç­†è³‡æ–™ã€‚çµæœå·²å„²å­˜è‡³ {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64c9ac11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed results saved to llama_ver_RQ3_evaluation_results.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def process_evaluation_results(input_filename=\"llama_ver_RQ3_evaluation_results.jsonl\",\n",
    "                               output_filename=\"llama_ver_RQ3_evaluation_results.jsonl\"):\n",
    "    \"\"\"\n",
    "    è™•ç†è©•ä¼°çµæœ JSONL æª”æ¡ˆï¼Œå°‡ \"custom_id\" æ‹†åˆ†ç‚º \"system\", \"model\", \"question_id\"ã€‚\n",
    "\n",
    "    Args:\n",
    "        input_filename (str): è¼¸å…¥çš„ JSONL æª”æ¡ˆåã€‚\n",
    "        output_filename (str): è¼¸å‡ºçš„ JSONL æª”æ¡ˆåã€‚\n",
    "    \"\"\"\n",
    "    processed_records = []\n",
    "    with open(input_filename, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            try:\n",
    "                record = json.loads(line.strip())\n",
    "                custom_id = record.pop(\"custom_id\", None)  # ç§»é™¤ \"custom_id\"ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡ç‚º None\n",
    "\n",
    "                if custom_id:\n",
    "                    parts = custom_id.split('-')\n",
    "                    record[\"question_id\"] = parts[0]\n",
    "\n",
    "                    if \"gpt-4o\" in custom_id:\n",
    "                        record[\"system\"] = parts[1]\n",
    "                        record[\"model\"] = parts[2] + \"-\" + parts[3]\n",
    "                    else:\n",
    "                        record[\"system\"] = parts[1]\n",
    "                        record[\"model\"] = parts[2]\n",
    "\n",
    "                processed_records.append(record)\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON line: {line.strip()} - {e}\")\n",
    "            except IndexError as e:\n",
    "                print(f\"Error splitting custom_id in line: {line.strip()} - {e}\")\n",
    "\n",
    "    with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
    "        for record in processed_records:\n",
    "            outfile.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"Processed results saved to {output_filename}\")\n",
    "\n",
    "\n",
    "process_evaluation_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80373461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å…±è™•ç† 500 ç­†\n",
      "âœ… æˆåŠŸè§£æè©•åˆ†çš„è³‡æ–™ï¼š500 ç­†\n",
      "âŒ æœªèƒ½è§£æè©•åˆ†çš„è³‡æ–™ï¼š0 ç­†\n",
      "\n",
      "ğŸ“Œ å‰å¹¾ç­†æœªèƒ½æˆåŠŸè§£æè©•åˆ†çš„ custom_idï¼š\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ast\n",
    "import re\n",
    "\n",
    "input_file = 'llama_ver_RQ3_evaluation_results.jsonl'\n",
    "output_file = 'llama_ver_RQ3_evaluation_results.jsonl' # æ›´æ”¹è¼¸å‡ºæª”æ¡ˆåä»¥å€åˆ¥\n",
    "\n",
    "# è©•åˆ†æ¬„ä½æ¨™æº–\n",
    "expected_fields = {\n",
    "    'Factuality', 'User Satisfaction', 'Clarity', 'Logical Coherence', 'Completeness', 'Final Score'\n",
    "}\n",
    "\n",
    "valid_scores = set(range(1, 11))\n",
    "\n",
    "processed_entries = [] # å„²å­˜æ‰€æœ‰è™•ç†éçš„æ¢ç›®\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        content = entry.get(\"content\", \"\")\n",
    "        score_dict = None\n",
    "\n",
    "        try:\n",
    "            matches = re.findall(r\"\\{[^{}]+\\}\", content)\n",
    "            for match in matches:\n",
    "                try:\n",
    "                    possible_dict = ast.literal_eval(match)\n",
    "                    if isinstance(possible_dict, dict) and len(possible_dict) == 6:\n",
    "                        keys = set(possible_dict.keys())\n",
    "                        values = set(possible_dict.values())\n",
    "                        if keys == expected_fields and all(isinstance(v, int) and v in valid_scores for v in possible_dict.values()):\n",
    "                            score_dict = possible_dict\n",
    "                            break\n",
    "                except (SyntaxError, ValueError):\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ç„¡æ³•è§£æ contentï¼š{e}\")\n",
    "\n",
    "        # åŠ å…¥ score æ¬„ä½ï¼ˆå¦‚æœæˆåŠŸè§£æï¼‰\n",
    "        if score_dict:\n",
    "            entry[\"score\"] = score_dict\n",
    "\n",
    "        processed_entries.append(entry) # å°‡è™•ç†å¾Œçš„ï¼ˆç„¡è«–æˆåŠŸèˆ‡å¦ï¼‰æ¢ç›®åŠ å…¥åˆ—è¡¨\n",
    "\n",
    "# å¯«å…¥è™•ç†å¾Œçš„æª”æ¡ˆï¼ˆåŒ…å«æˆåŠŸå’Œå¤±æ•—çš„ï¼‰\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for entry in processed_entries:\n",
    "        json.dump(entry, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "# é¡¯ç¤ºå ±å‘Š\n",
    "valid_count = sum(1 for entry in processed_entries if \"score\" in entry)\n",
    "invalid_count = len(processed_entries) - valid_count\n",
    "print(f\"âœ… å…±è™•ç† {len(processed_entries)} ç­†\")\n",
    "print(f\"âœ… æˆåŠŸè§£æè©•åˆ†çš„è³‡æ–™ï¼š{valid_count} ç­†\")\n",
    "print(f\"âŒ æœªèƒ½è§£æè©•åˆ†çš„è³‡æ–™ï¼š{invalid_count} ç­†\")\n",
    "\n",
    "# å¯é¸ï¼šåˆ—å‡ºå‰å¹¾ç­†æœªèƒ½è§£æè©•åˆ†çš„ custom_id ä¾†æª¢æŸ¥åŸå› \n",
    "print(\"\\nğŸ“Œ å‰å¹¾ç­†æœªèƒ½æˆåŠŸè§£æè©•åˆ†çš„ custom_idï¼š\")\n",
    "for item in processed_entries:\n",
    "    if \"score\" not in item:\n",
    "        print(\"-\", item.get(\"system\"))\n",
    "        print(\"-\", item.get(\"model\"))\n",
    "        print(\"-\", item.get(\"question_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e35eba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š å„ç³»çµ± + æ¨¡å‹ çš„å¹³å‡æˆç¸¾ï¼š\n",
      "\n",
      "ğŸ”¹ ç³»çµ±ï¼šReAct | æ¨¡å‹ï¼šllama 3.3 70B\n",
      "  Factuality: 6.47\n",
      "  User Satisfaction: 4.87\n",
      "  Clarity: 7.09\n",
      "  Logical Coherence: 6.43\n",
      "  Completeness: 4.09\n",
      "  Final Score: 5.15\n",
      "\n",
      "ğŸ”¹ ç³»çµ±ï¼šCQ_Solver | æ¨¡å‹ï¼šllama 3.3 70B\n",
      "  Factuality: 7.78\n",
      "  User Satisfaction: 6.57\n",
      "  Clarity: 8.13\n",
      "  Logical Coherence: 8.11\n",
      "  Completeness: 5.92\n",
      "  Final Score: 6.82\n",
      "\n",
      "ğŸ”¹ ç³»çµ±ï¼šReAct | æ¨¡å‹ï¼šgpt-4o\n",
      "  Factuality: 8.13\n",
      "  User Satisfaction: 7.58\n",
      "  Clarity: 8.54\n",
      "  Logical Coherence: 8.53\n",
      "  Completeness: 7.14\n",
      "  Final Score: 7.66\n",
      "\n",
      "ğŸ”¹ ç³»çµ±ï¼šCQ_Solver | æ¨¡å‹ï¼šgpt-4o\n",
      "  Factuality: 8.44\n",
      "  User Satisfaction: 8.00\n",
      "  Clarity: 8.67\n",
      "  Logical Coherence: 8.84\n",
      "  Completeness: 7.77\n",
      "  Final Score: 8.14\n",
      "\n",
      "ğŸ”¹ ç³»çµ±ï¼šMindSearch | æ¨¡å‹ï¼šllama 3.3 70B\n",
      "  Factuality: 7.80\n",
      "  User Satisfaction: 6.71\n",
      "  Clarity: 8.09\n",
      "  Logical Coherence: 8.09\n",
      "  Completeness: 6.16\n",
      "  Final Score: 6.97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "input_file = \"processed_evaluation_results.jsonl\" #'./result/final_evaluation_results.jsonl'\n",
    "\n",
    "expected_fields = [\n",
    "    'Factuality', 'User Satisfaction', 'Clarity',\n",
    "    'Logical Coherence', 'Completeness', 'Final Score'\n",
    "]\n",
    "\n",
    "# åˆå§‹åŒ–çµ±è¨ˆè³‡æ–™: (system, model) â†’ å„è©•åˆ†æ¬„ä½çš„æ•¸å€¼ list\n",
    "systems_scores = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# è®€å–ä¸¦åˆ†é¡ç´¯åŠ \n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        system = entry.get(\"system\")\n",
    "        model = entry.get(\"model\")\n",
    "        score = entry.get(\"score\")\n",
    "\n",
    "        if system and model and score:\n",
    "            key = (system, model)\n",
    "            for field in expected_fields:\n",
    "                value = score.get(field)\n",
    "                if isinstance(value, int):\n",
    "                    systems_scores[key][field].append(value)\n",
    "\n",
    "# è¨ˆç®—å¹³å‡åˆ†æ•¸\n",
    "print(\"\\nğŸ“Š å„ç³»çµ± + æ¨¡å‹ çš„å¹³å‡æˆç¸¾ï¼š\\n\")\n",
    "for (system, model), scores in systems_scores.items():\n",
    "    print(f\"ğŸ”¹ ç³»çµ±ï¼š{system} | æ¨¡å‹ï¼š{model}\")\n",
    "    for field in expected_fields:\n",
    "        values = scores[field]\n",
    "        if values:\n",
    "            avg = sum(values) / len(values)\n",
    "            print(f\"  {field}: {avg:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {field}: ç„¡è³‡æ–™\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d752bc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å°‡åˆä½µçµæœå¯«å…¥æª”æ¡ˆï¼šRQ3_llama_best_After_CD.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def fetch_original_results(refined_results_file=\"final_refined_evaluation_results.jsonl\",\n",
    "                           original_results_files={\n",
    "                               1: \"RQ3_experiment_results.jsonl\",\n",
    "                               2: \"RQ3_experiment_results_2.jsonl\",\n",
    "                               3: \"RQ3_experiment_results_3.jsonl\"\n",
    "                           },\n",
    "                           output_file=\"RQ3_llama_best_After_CD.jsonl\"):\n",
    "    \"\"\"\n",
    "    æ ¹æ“š \"__order\" æ¬„ä½å¾ä¸åŒçš„åŸå§‹çµæœæª”æ¡ˆä¸­å–å›å°æ‡‰çš„è¡Œï¼Œä¸¦åˆä½µåˆ°ä¸€å€‹æ–°çš„ JSONL æª”æ¡ˆä¸­ã€‚\n",
    "\n",
    "    Args:\n",
    "        refined_results_file (str): åŒ…å« \"__order\" æ¬„ä½çš„ JSONL æª”æ¡ˆã€‚\n",
    "        original_results_files (dict): åŒ…å« \"__order\" å€¼èˆ‡å°æ‡‰åŸå§‹çµæœæª”æ¡ˆåçš„å­—å…¸ã€‚\n",
    "        output_file (str): è¼¸å‡ºåˆä½µçµæœçš„ JSONL æª”æ¡ˆåã€‚\n",
    "    \"\"\"\n",
    "    original_data = {}\n",
    "    for order, filename in original_results_files.items():\n",
    "        original_data[order] = {}\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    record = json.loads(line)\n",
    "                    key = (record.get(\"system\"), record.get(\"model\"), record.get(\"question_id\"))\n",
    "                    original_data[order][key] = record\n",
    "        except FileNotFoundError:\n",
    "            print(f\"è­¦å‘Šï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ {filename}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"è­¦å‘Šï¼šè§£ææª”æ¡ˆ {filename} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")\n",
    "\n",
    "    merged_results = []\n",
    "    try:\n",
    "        with open(refined_results_file, 'r', encoding='utf-8') as f_refined:\n",
    "            for line_refined in f_refined:\n",
    "                record_refined = json.loads(line_refined)\n",
    "                order = record_refined.get(\"__order\")\n",
    "                system_refined = record_refined.get(\"system\")\n",
    "                model_refined = record_refined.get(\"model\")\n",
    "                question_id_refined = record_refined.get(\"question_id\")\n",
    "\n",
    "                if order in original_data:\n",
    "                    key_refined = (system_refined, model_refined, question_id_refined)\n",
    "                    if key_refined in original_data[order]:\n",
    "                        merged_results.append(original_data[order][key_refined])\n",
    "                    else:\n",
    "                        print(f\"è­¦å‘Šï¼šåœ¨æª”æ¡ˆ {original_results_files[order]} ä¸­æ‰¾ä¸åˆ°èˆ‡ {system_refined}, {model_refined}, {question_id_refined} ç›¸ç¬¦çš„è¨˜éŒ„ (order: {order})\")\n",
    "                        merged_results.append(record_refined) # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä¿ç•™ refined çš„è¨˜éŒ„\n",
    "                else:\n",
    "                    print(f\"è­¦å‘Šï¼š__order å€¼ {order} ç„¡å°æ‡‰çš„åŸå§‹çµæœæª”æ¡ˆã€‚\")\n",
    "                    merged_results.append(record_refined) # å¦‚æœ order ä¸åœ¨å­—å…¸ä¸­ï¼Œä¿ç•™ refined çš„è¨˜éŒ„\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ {refined_results_file}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"éŒ¯èª¤ï¼šè§£ææª”æ¡ˆ {refined_results_file} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")\n",
    "\n",
    "    # å¯«å…¥æ–°çš„ JSONL æª”æ¡ˆ\n",
    "    with open(output_file, 'w', encoding='utf-8') as f_output:\n",
    "        for record in merged_results:\n",
    "            json.dump(record, f_output, ensure_ascii=False)\n",
    "            f_output.write('\\n')\n",
    "\n",
    "    print(f\"å·²å°‡åˆä½µçµæœå¯«å…¥æª”æ¡ˆï¼š{output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_original_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jinqi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
