{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a5424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "eval_prompt_p1 = \"\"\"You are asked to assess the quality of an AI assistant's answer to a user's question as an impartial judge. Since the type of answer you are evaluating is [Solve Professional Problem], you need to evaluate the answer in the following 5 criteria:\n",
    "1. Factuality: Whether the information provided is accurate and based on reliable facts and data.\n",
    "2. User Satisfaction: Whether the response meets the user's question and needs and provides a comprehensive and appropriate answer to the question.\n",
    "3. Clarity: Whether the response is clear and understandable, and whether it uses concise language and structure so that the user can easily understand it.\n",
    "4. Logical Coherence: Whether the response maintains overall consistency and logical coherence between different sections, avoiding self-contradiction.\n",
    "5. Completeness: Whether the response provides sufficient information and details to meet the user's needs, and whether it avoids omitting important aspects.\n",
    "6. Note that a longer answer is not always better, the answer that is concise and meets the above requirements is the best.\n",
    "\n",
    "We will provide you with the user's question, an 8-score reference answer, and answers from the AI assistant that needs your assessment. When starting your evaluation, you need to follow the reasoning steps below:\n",
    "1. Compare the AI assistant's answer with the reference answer, point out any shortcomings in the AI assistant's answer, and explain further.\n",
    "2. Evaluate the AI assistant's answer in terms of the different criteria, giving each criterion a score from 1 to 10 after the evaluation of each.\n",
    "3. Finally, combine the evaluations from each criterion and give the AI assistant's answer a composite score of 1 to 10.\n",
    "4. Your scoring needs to be as rigorous as possible and adhere to the following scoring rules: in general, the higher the quality of the model's answers, the higher the score.\n",
    "The two most important criteria are factual correctness and fulfillment of user needs, and the scores for these two dimensions dominate the final composite score.\n",
    "\n",
    "When the model answer has irrelevance to the question, or intrinsically factually incorrect, or generates harmful content, the total score should be 1 to 2;\n",
    "When the model answer has no serious errors and is largely harmless, but is of low quality and does not meet user requirements, the total score must be 3 to 4;\n",
    "When the model answer basically meets the user's needs but performs poorly on some criteria and is of medium quality, the total score can be 5 to 6;\n",
    "When the quality of the model response is similar to the reference answer and performs well in all criteria, the total score should be 7 to 8;\n",
    "A score of 9 to 10 can only be achieved if the model significantly exceeds the quality of the reference answer, adequately addresses the user's question and all the needs, and is close to a perfect score on all criteria. As an example, the reference answer would receive a score of 8.\n",
    "\n",
    "You need to evaluate and explain before you score. Your explanation of each criterion needs to be followed by the scoring. After that, at the end of your answer, return all of your scores in the following dictionary format, including the curly brackets, and make sure that your scores are integers:\n",
    "{'Dimension 1': scoring, 'Dimension 2': scoring, ... , 'Final Score': Score}, e.g. {'Factuality': 9, 'User Satisfaction': 6, ... , 'Final Score': 7}.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41002d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input file created: llama_ver_batch_evaluation_RQ3.jsonl\n",
      "Batch input file uploaded with ID: file_01jtzxks0jfwm8jnvgvp4t9g7b\n",
      "Batch created with ID: batch_01jtzxksbkeevbxsjcqx5ytc7n\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "def prepare_batch_evaluation(results_file=\"./result/final_experiment_results.jsonl\",\n",
    "                             output_batch_file=\"batch_evaluation.jsonl\"):\n",
    "    \"\"\"\n",
    "    準備 Batch API 輸入檔案，以評估 final_experiment_results.jsonl 中的答案。\n",
    "    使用 system == \"MindSearch\" 和 model == \"gpt-4o\" 的答案作為參考答案。\n",
    "    評估所有其他模型和系統組合的答案。\n",
    "    \"\"\"\n",
    "    batch_requests = []\n",
    "    reference_answers = {}\n",
    "    data_to_evaluate = {}\n",
    "    \n",
    "    RQ3_llama_best_After_CD = \"final_RQ3_experiment_results.jsonl\"\n",
    "\n",
    "    # 載入參考答案 (MindSearch, gpt-4o)\n",
    "    with open(results_file, 'r') as f_ref:\n",
    "        for line in f_ref:\n",
    "            try:\n",
    "                record = json.loads(line.strip())\n",
    "                if record.get(\"system\") == \"MindSearch\" and record.get(\"model\") == \"gpt-4o\" and record.get(\"question_id\"):\n",
    "                    reference_answers[record[\"question_id\"]] = {\n",
    "                        \"answer\": record.get(\"answer\"),\n",
    "                        \"question\": record.get(\"question\")\n",
    "                    }\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding reference answers: {e}\")\n",
    "                continue\n",
    "\n",
    "    # 載入需要評估的答案 (所有其他組合)\n",
    "    with open(RQ3_llama_best_After_CD, 'r') as f_eval:\n",
    "        for line in f_eval:\n",
    "            try:\n",
    "                record = json.loads(line.strip())\n",
    "                question_id = record.get(\"question_id\")\n",
    "                system = record.get(\"system\")\n",
    "                model = record.get(\"model\")\n",
    "                answer = record.get(\"answer\")\n",
    "                question = record.get(\"question\")\n",
    "\n",
    "                if question_id and answer is not None and not (system == \"MindSearch\" and model == \"gpt-4o\") and question_id in reference_answers:\n",
    "                    if question_id not in data_to_evaluate:\n",
    "                        data_to_evaluate[question_id] = {}\n",
    "                    data_to_evaluate[question_id][f\"{system}-{model}\"] = answer\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding evaluation data: {e}\")\n",
    "                continue\n",
    "            \n",
    "    print(f\"Loaded {len(reference_answers)} reference answers.\")\n",
    "    print(f\"Attempting to evaluate answers for {len(data_to_evaluate)} question IDs.\")\n",
    "\n",
    "    evaluated_count = 0\n",
    "    skipped_due_to_no_reference = 0\n",
    "\n",
    "    # 準備 Batch API 請求\n",
    "    for question_id, answers_by_model in data_to_evaluate.items():\n",
    "        reference_info = reference_answers.get(question_id)\n",
    "        if reference_info:\n",
    "            reference_answer = reference_info[\"answer\"]\n",
    "            question = reference_info[\"question\"]\n",
    "            for model_system, evaluated_answer in answers_by_model.items():\n",
    "                try:\n",
    "                    system, model = model_system.split('-', 1)\n",
    "                    custom_id = f\"{question_id}-{system}-{model}\"\n",
    "\n",
    "                    eval_prompt_p2 = f\"\"\"Question: \"{question}\"\n",
    "<Reference Answer>\n",
    "{reference_answer}\n",
    "</Reference Answer>\n",
    "\n",
    "<AI assistant's answer>\n",
    "{evaluated_answer}\n",
    "</AI assistant's answer>\"\"\"\n",
    "\n",
    "                    batch_requests.append({\n",
    "                        \"custom_id\": custom_id,\n",
    "                        \"method\": \"POST\",\n",
    "                        \"url\": \"/v1/chat/completions\",\n",
    "                        \"body\": {\n",
    "                            \"model\": \"llama-3.3-70b-versatile\",\n",
    "                            \"messages\": [{\"role\": \"system\", \"content\": eval_prompt_p1},\n",
    "                                         {\"role\": \"user\", \"content\": eval_prompt_p2}]\n",
    "                        }\n",
    "                    })\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error splitting model_system '{model_system}': {e}\")\n",
    "                    continue # 跳過這個有問題的 model_system\n",
    "                \n",
    "            evaluated_count += len(answers_by_model)\n",
    "            \n",
    "        else:\n",
    "            skipped_due_to_no_reference += len(answers_by_model)\n",
    "            print(f\"Warning: No reference answer found for question_id '{question_id}'. Skipping evaluation for: {answers_by_model.keys()}\")\n",
    "    print(f\"Created {len(batch_requests)} evaluation requests.\")\n",
    "    print(f\"Skipped {skipped_due_to_no_reference} evaluations due to missing reference answers.\")\n",
    "\n",
    "    # 將請求寫入 Batch 輸入檔案\n",
    "    with open(output_batch_file, 'w') as f:\n",
    "        for req in batch_requests:\n",
    "            f.write(json.dumps(req) + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "    output_batch_file = \"llama_ver_batch_evaluation_RQ3.jsonl\"\n",
    "    # prepare_batch_evaluation(output_batch_file=output_batch_file)\n",
    "    print(f\"Batch input file created: {output_batch_file}\")\n",
    "\n",
    "    # 步驟 2: 上傳 Batch 輸入檔案\n",
    "    try:\n",
    "        with open(output_batch_file, \"rb\") as f:\n",
    "            batch_input_file = client.files.create(\n",
    "                file=f,\n",
    "                purpose=\"batch\"\n",
    "            )\n",
    "        print(f\"Batch input file uploaded with ID: {batch_input_file.id}\")\n",
    "        input_file_id = batch_input_file.id\n",
    "\n",
    "        # 步驟 3: 創建 Batch\n",
    "        batch = client.batches.create(\n",
    "            input_file_id=input_file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\"description\": \"Batch evaluation of different model/system combinations\"}\n",
    "        )\n",
    "        print(f\"Batch created with ID: {batch.id}\")\n",
    "\n",
    "        # **後續步驟：輪詢狀態、檢索結果、解析和記錄結果**\n",
    "        # 您需要實現這些步驟，就像之前的流程一樣。\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Batch API interaction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d057618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchRetrieveResponse(id='batch_01jtzxksbkeevbxsjcqx5ytc7n', completion_window='24h', created_at=1746975450, endpoint='/v1/chat/completions', input_file_id='file_01jtzxks0jfwm8jnvgvp4t9g7b', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1746975470, error_file_id=None, errors=None, expired_at=None, expires_at=1747061850, failed_at=None, finalizing_at=1746975470, in_progress_at=1746975454, metadata={'description': 'Batch evaluation of different model/system combinations'}, output_file_id='file_01jtzxmckhew9ve80v2fqgbswv', request_counts=RequestCounts(completed=500, failed=0, total=500))\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "client = Groq()\n",
    "\n",
    "batch = client.batches.retrieve(\"batch_01jtzxksbkeevbxsjcqx5ytc7n\")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0df94699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "client = Groq()\n",
    "\n",
    "file_response = client.files.content(\"file_01jtzxmckhew9ve80v2fqgbswv\")\n",
    "output_filename = \"llama_ver_RQ3_evaluation_results.jsonl\"\n",
    "file_response.write_to_file(output_filename)\n",
    "# with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
    "#     outfile.write(file_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b43a917d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 完成，共處理 500 筆資料。結果已儲存至 llama_ver_RQ3_evaluation_results.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_file = 'llama_ver_RQ3_evaluation_results.jsonl'\n",
    "output_file = 'llama_ver_RQ3_evaluation_results.jsonl'\n",
    "\n",
    "cleaned_data = []\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            entry = json.loads(line)\n",
    "            custom_id = entry.get(\"custom_id\")\n",
    "            content = (\n",
    "                entry.get(\"response\", {})\n",
    "                     .get(\"body\", {})\n",
    "                     .get(\"choices\", [{}])[0]\n",
    "                     .get(\"message\", {})\n",
    "                     .get(\"content\", \"\")\n",
    "            )\n",
    "            cleaned_data.append({\n",
    "                \"custom_id\": custom_id,\n",
    "                \"content\": content\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 處理失敗: {e}\")\n",
    "            continue\n",
    "\n",
    "# ✅ 可選：寫入清理後的 .jsonl 檔案\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for item in cleaned_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"✅ 完成，共處理 {len(cleaned_data)} 筆資料。結果已儲存至 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64c9ac11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed results saved to llama_ver_RQ3_evaluation_results.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def process_evaluation_results(input_filename=\"llama_ver_RQ3_evaluation_results.jsonl\",\n",
    "                               output_filename=\"llama_ver_RQ3_evaluation_results.jsonl\"):\n",
    "    \"\"\"\n",
    "    處理評估結果 JSONL 檔案，將 \"custom_id\" 拆分為 \"system\", \"model\", \"question_id\"。\n",
    "\n",
    "    Args:\n",
    "        input_filename (str): 輸入的 JSONL 檔案名。\n",
    "        output_filename (str): 輸出的 JSONL 檔案名。\n",
    "    \"\"\"\n",
    "    processed_records = []\n",
    "    with open(input_filename, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            try:\n",
    "                record = json.loads(line.strip())\n",
    "                custom_id = record.pop(\"custom_id\", None)  # 移除 \"custom_id\"，如果不存在則為 None\n",
    "\n",
    "                if custom_id:\n",
    "                    parts = custom_id.split('-')\n",
    "                    record[\"question_id\"] = parts[0]\n",
    "\n",
    "                    if \"gpt-4o\" in custom_id:\n",
    "                        record[\"system\"] = parts[1]\n",
    "                        record[\"model\"] = parts[2] + \"-\" + parts[3]\n",
    "                    else:\n",
    "                        record[\"system\"] = parts[1]\n",
    "                        record[\"model\"] = parts[2]\n",
    "\n",
    "                processed_records.append(record)\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON line: {line.strip()} - {e}\")\n",
    "            except IndexError as e:\n",
    "                print(f\"Error splitting custom_id in line: {line.strip()} - {e}\")\n",
    "\n",
    "    with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
    "        for record in processed_records:\n",
    "            outfile.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"Processed results saved to {output_filename}\")\n",
    "\n",
    "\n",
    "process_evaluation_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80373461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 共處理 500 筆\n",
      "✅ 成功解析評分的資料：500 筆\n",
      "❌ 未能解析評分的資料：0 筆\n",
      "\n",
      "📌 前幾筆未能成功解析評分的 custom_id：\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ast\n",
    "import re\n",
    "\n",
    "input_file = 'llama_ver_RQ3_evaluation_results.jsonl'\n",
    "output_file = 'llama_ver_RQ3_evaluation_results.jsonl' # 更改輸出檔案名以區別\n",
    "\n",
    "# 評分欄位標準\n",
    "expected_fields = {\n",
    "    'Factuality', 'User Satisfaction', 'Clarity', 'Logical Coherence', 'Completeness', 'Final Score'\n",
    "}\n",
    "\n",
    "valid_scores = set(range(1, 11))\n",
    "\n",
    "processed_entries = [] # 儲存所有處理過的條目\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        content = entry.get(\"content\", \"\")\n",
    "        score_dict = None\n",
    "\n",
    "        try:\n",
    "            matches = re.findall(r\"\\{[^{}]+\\}\", content)\n",
    "            for match in matches:\n",
    "                try:\n",
    "                    possible_dict = ast.literal_eval(match)\n",
    "                    if isinstance(possible_dict, dict) and len(possible_dict) == 6:\n",
    "                        keys = set(possible_dict.keys())\n",
    "                        values = set(possible_dict.values())\n",
    "                        if keys == expected_fields and all(isinstance(v, int) and v in valid_scores for v in possible_dict.values()):\n",
    "                            score_dict = possible_dict\n",
    "                            break\n",
    "                except (SyntaxError, ValueError):\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 無法解析 content：{e}\")\n",
    "\n",
    "        # 加入 score 欄位（如果成功解析）\n",
    "        if score_dict:\n",
    "            entry[\"score\"] = score_dict\n",
    "\n",
    "        processed_entries.append(entry) # 將處理後的（無論成功與否）條目加入列表\n",
    "\n",
    "# 寫入處理後的檔案（包含成功和失敗的）\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for entry in processed_entries:\n",
    "        json.dump(entry, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "# 顯示報告\n",
    "valid_count = sum(1 for entry in processed_entries if \"score\" in entry)\n",
    "invalid_count = len(processed_entries) - valid_count\n",
    "print(f\"✅ 共處理 {len(processed_entries)} 筆\")\n",
    "print(f\"✅ 成功解析評分的資料：{valid_count} 筆\")\n",
    "print(f\"❌ 未能解析評分的資料：{invalid_count} 筆\")\n",
    "\n",
    "# 可選：列出前幾筆未能解析評分的 custom_id 來檢查原因\n",
    "print(\"\\n📌 前幾筆未能成功解析評分的 custom_id：\")\n",
    "for item in processed_entries:\n",
    "    if \"score\" not in item:\n",
    "        print(\"-\", item.get(\"system\"))\n",
    "        print(\"-\", item.get(\"model\"))\n",
    "        print(\"-\", item.get(\"question_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e35eba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 各系統 + 模型 的平均成績：\n",
      "\n",
      "🔹 系統：ReAct | 模型：llama 3.3 70B\n",
      "  Factuality: 6.47\n",
      "  User Satisfaction: 4.87\n",
      "  Clarity: 7.09\n",
      "  Logical Coherence: 6.43\n",
      "  Completeness: 4.09\n",
      "  Final Score: 5.15\n",
      "\n",
      "🔹 系統：CQ_Solver | 模型：llama 3.3 70B\n",
      "  Factuality: 7.78\n",
      "  User Satisfaction: 6.57\n",
      "  Clarity: 8.13\n",
      "  Logical Coherence: 8.11\n",
      "  Completeness: 5.92\n",
      "  Final Score: 6.82\n",
      "\n",
      "🔹 系統：ReAct | 模型：gpt-4o\n",
      "  Factuality: 8.13\n",
      "  User Satisfaction: 7.58\n",
      "  Clarity: 8.54\n",
      "  Logical Coherence: 8.53\n",
      "  Completeness: 7.14\n",
      "  Final Score: 7.66\n",
      "\n",
      "🔹 系統：CQ_Solver | 模型：gpt-4o\n",
      "  Factuality: 8.44\n",
      "  User Satisfaction: 8.00\n",
      "  Clarity: 8.67\n",
      "  Logical Coherence: 8.84\n",
      "  Completeness: 7.77\n",
      "  Final Score: 8.14\n",
      "\n",
      "🔹 系統：MindSearch | 模型：llama 3.3 70B\n",
      "  Factuality: 7.80\n",
      "  User Satisfaction: 6.71\n",
      "  Clarity: 8.09\n",
      "  Logical Coherence: 8.09\n",
      "  Completeness: 6.16\n",
      "  Final Score: 6.97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "input_file = \"processed_evaluation_results.jsonl\" #'./result/final_evaluation_results.jsonl'\n",
    "\n",
    "expected_fields = [\n",
    "    'Factuality', 'User Satisfaction', 'Clarity',\n",
    "    'Logical Coherence', 'Completeness', 'Final Score'\n",
    "]\n",
    "\n",
    "# 初始化統計資料: (system, model) → 各評分欄位的數值 list\n",
    "systems_scores = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# 讀取並分類累加\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        system = entry.get(\"system\")\n",
    "        model = entry.get(\"model\")\n",
    "        score = entry.get(\"score\")\n",
    "\n",
    "        if system and model and score:\n",
    "            key = (system, model)\n",
    "            for field in expected_fields:\n",
    "                value = score.get(field)\n",
    "                if isinstance(value, int):\n",
    "                    systems_scores[key][field].append(value)\n",
    "\n",
    "# 計算平均分數\n",
    "print(\"\\n📊 各系統 + 模型 的平均成績：\\n\")\n",
    "for (system, model), scores in systems_scores.items():\n",
    "    print(f\"🔹 系統：{system} | 模型：{model}\")\n",
    "    for field in expected_fields:\n",
    "        values = scores[field]\n",
    "        if values:\n",
    "            avg = sum(values) / len(values)\n",
    "            print(f\"  {field}: {avg:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {field}: 無資料\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d752bc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已將合併結果寫入檔案：RQ3_llama_best_After_CD.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def fetch_original_results(refined_results_file=\"final_refined_evaluation_results.jsonl\",\n",
    "                           original_results_files={\n",
    "                               1: \"RQ3_experiment_results.jsonl\",\n",
    "                               2: \"RQ3_experiment_results_2.jsonl\",\n",
    "                               3: \"RQ3_experiment_results_3.jsonl\"\n",
    "                           },\n",
    "                           output_file=\"RQ3_llama_best_After_CD.jsonl\"):\n",
    "    \"\"\"\n",
    "    根據 \"__order\" 欄位從不同的原始結果檔案中取回對應的行，並合併到一個新的 JSONL 檔案中。\n",
    "\n",
    "    Args:\n",
    "        refined_results_file (str): 包含 \"__order\" 欄位的 JSONL 檔案。\n",
    "        original_results_files (dict): 包含 \"__order\" 值與對應原始結果檔案名的字典。\n",
    "        output_file (str): 輸出合併結果的 JSONL 檔案名。\n",
    "    \"\"\"\n",
    "    original_data = {}\n",
    "    for order, filename in original_results_files.items():\n",
    "        original_data[order] = {}\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    record = json.loads(line)\n",
    "                    key = (record.get(\"system\"), record.get(\"model\"), record.get(\"question_id\"))\n",
    "                    original_data[order][key] = record\n",
    "        except FileNotFoundError:\n",
    "            print(f\"警告：找不到檔案 {filename}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"警告：解析檔案 {filename} 時發生錯誤：{e}\")\n",
    "\n",
    "    merged_results = []\n",
    "    try:\n",
    "        with open(refined_results_file, 'r', encoding='utf-8') as f_refined:\n",
    "            for line_refined in f_refined:\n",
    "                record_refined = json.loads(line_refined)\n",
    "                order = record_refined.get(\"__order\")\n",
    "                system_refined = record_refined.get(\"system\")\n",
    "                model_refined = record_refined.get(\"model\")\n",
    "                question_id_refined = record_refined.get(\"question_id\")\n",
    "\n",
    "                if order in original_data:\n",
    "                    key_refined = (system_refined, model_refined, question_id_refined)\n",
    "                    if key_refined in original_data[order]:\n",
    "                        merged_results.append(original_data[order][key_refined])\n",
    "                    else:\n",
    "                        print(f\"警告：在檔案 {original_results_files[order]} 中找不到與 {system_refined}, {model_refined}, {question_id_refined} 相符的記錄 (order: {order})\")\n",
    "                        merged_results.append(record_refined) # 如果找不到，保留 refined 的記錄\n",
    "                else:\n",
    "                    print(f\"警告：__order 值 {order} 無對應的原始結果檔案。\")\n",
    "                    merged_results.append(record_refined) # 如果 order 不在字典中，保留 refined 的記錄\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"錯誤：找不到檔案 {refined_results_file}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"錯誤：解析檔案 {refined_results_file} 時發生錯誤：{e}\")\n",
    "\n",
    "    # 寫入新的 JSONL 檔案\n",
    "    with open(output_file, 'w', encoding='utf-8') as f_output:\n",
    "        for record in merged_results:\n",
    "            json.dump(record, f_output, ensure_ascii=False)\n",
    "            f_output.write('\\n')\n",
    "\n",
    "    print(f\"已將合併結果寫入檔案：{output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_original_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jinqi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
